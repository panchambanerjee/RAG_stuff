{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective: To build a simple RAG evaluation framework\n",
    "\n",
    "### Part 1: Synthesize and filter an Instruction dataset from a custom knowledge-base\n",
    "\n",
    "#### Primary reference: https://huggingface.co/learn/cookbook/en/rag_evaluation by Aymeric Roucher (https://huggingface.co/m-ric)\n",
    "\n",
    "For the knowledge base, let us use the  litgpt Github repo: https://github.com/Lightning-AI/litgpt/tree/main -> Only use markdown files, this ensures the knowledge base isnt too large (as a first effort),  and we get the quickstart, tutorials etc. **should** mean coherent QAs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installs and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --quiet torch transformers langchain tqdm pandas datasets\n",
    "%pip install -U --quiet langchain-openai Gitpython python-dotenv huggingface_hub\n",
    "%pip install --quiet openai huggingface langchain_experimental sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "\n",
    "# openai_api_key = os.environ['OPENAI_API_KEY'] \n",
    "hf_api_key = os.environ['HF_API_KEY'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import json\n",
    "import datasets\n",
    "import random\n",
    "import bs4\n",
    "import glob\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import GitLoader\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.docstore.document import Document as LangchainDocument\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores.utils import DistanceStrategy\n",
    "from langchain_community.llms import HuggingFaceHub\n",
    "from langchain_core.vectorstores import VectorStore\n",
    "from langchain_core.language_models.llms import LLM\n",
    "from langchain_core.language_models import BaseChatModel\n",
    "\n",
    "\n",
    "from huggingface_hub import InferenceClient\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in knowledge base and prepare documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = GitLoader(\n",
    "    clone_url=\"https://github.com/Lightning-AI/litgpt\",\n",
    "    repo_path=\"./litgpt_data_github/\",\n",
    "    branch=\"main\",\n",
    "    file_filter=lambda file_path: file_path.endswith(\".md\") # Only get the markdown files\n",
    ")\n",
    "\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=2000,\n",
    "    chunk_overlap=200,\n",
    "    add_start_index=True,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\", \"\\n\\n\\n\"],\n",
    ")\n",
    "\n",
    "docs_processed = []\n",
    "for doc in data:\n",
    "    docs_processed += text_splitter.split_documents([doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Question-Answer generation agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_id = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
    "\n",
    "llm_client = InferenceClient(\n",
    "    model=repo_id,\n",
    "    timeout=120,\n",
    "    token=hf_api_key\n",
    ")\n",
    "\n",
    "\n",
    "def call_llm(inference_client: InferenceClient, prompt: str):\n",
    "    response = inference_client.post(\n",
    "        json={\n",
    "            \"inputs\": prompt,\n",
    "            \"parameters\": {\"max_new_tokens\": 1000},\n",
    "            \"task\": \"text-generation\",\n",
    "        },\n",
    "    )\n",
    "    return json.loads(response.decode())[0][\"generated_text\"]\n",
    "\n",
    "QA_generation_prompt = \"\"\"\n",
    "Your task is to write a factoid question and an answer given a context.\n",
    "Your factoid question should be answerable with a specific, concise piece of factual information from the context.\n",
    "Your factoid question should be formulated in the same style as questions users could ask in a search engine.\n",
    "This means that your factoid question MUST NOT mention something like \"according to the passage\" or \"context\".\n",
    "\n",
    "Provide your answer as follows:\n",
    "\n",
    "Output:::\n",
    "Factoid question: (your factoid question)\n",
    "Answer: (your answer to the factoid question)\n",
    "\n",
    "Now here is the context.\n",
    "\n",
    "Context: {context}\\n\n",
    "Output:::\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "136"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 136 QA couples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 136/136 [12:34<00:00,  5.55s/it]\n"
     ]
    }
   ],
   "source": [
    "# Generate 136 samples for now, upload to Huggingface Hub to use later\n",
    "\n",
    "\n",
    "N_GENERATIONS = len(docs_processed)  # We intentionally generate only 136 QA couples here for cost and time considerations\n",
    "\n",
    "# This number is just the total length of docs_processed using the knowledge-base from litgpt markdown files\n",
    "\n",
    "print(f\"Generating {N_GENERATIONS} QA couples...\")\n",
    "\n",
    "outputs = []\n",
    "for sampled_context in tqdm(random.sample(docs_processed, N_GENERATIONS)):\n",
    "    # Generate QA couple\n",
    "    output_QA_couple = call_llm(\n",
    "        llm_client, QA_generation_prompt.format(context=sampled_context.page_content)\n",
    "    )\n",
    "    try:\n",
    "        question = output_QA_couple.split(\"Factoid question: \")[-1].split(\"Answer: \")[0]\n",
    "        answer = output_QA_couple.split(\"Answer: \")[-1]\n",
    "        assert len(answer) < 300, \"Answer is too long\"\n",
    "        outputs.append(\n",
    "            {\n",
    "                \"context\": sampled_context.page_content,\n",
    "                \"question\": question,\n",
    "                \"answer\": answer,\n",
    "                \"source_doc\": sampled_context.metadata[\"source\"],\n",
    "            }\n",
    "        )\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>source_doc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td># TPU support\\n\\nThis project utilizes [`Fabric`](https://lightning.ai/docs/fabric/stable), which supports TPUs via [PyTorch XLA](https://github.com/pytorch/xla).\\n\\n&gt; [!NOTE]\\n&gt; This guide assumes that you have already set-up your [Google Cloud environment](https://cloud.google.com/run/docs/setup).\\n\\nTo set up a Google Cloud instance with a TPU v4 VM, run the following commands:\\n\\n```shell\\ngcloud compute tpus tpu-vm create litgpt --version=tpu-vm-v4-base --accelerator-type=v4-8 --zone=us-central2-b\\ngcloud compute tpus tpu-vm ssh litgpt --zone=us-central2-b\\n```\\n\\nYou can also choose a different TPU type. To do so, change the `version`, `accelerator-type`, and `zone` arguments. Find all regions and zones [here](https://cloud.google.com/tpu/docs/regions-zones).\\n\\n&lt;details&gt;\\n&lt;summary&gt;Multihost caveats&lt;/summary&gt;\\n\\nTPU v4-8 uses a single host. SSH'ing into the machine and running commands manually will only work when using a single host (1 slice in the TPU pod).\\nIn multi-host environments, such as larger TPU pod slices, it's necessary to launch all commands on all hosts simultaneously to avoid hangs.\\nFor local development, it is advisable to upload a zip file containing all your current changes and execute it inside the VM from your personal computer:\\n\\n```shell\\n# Zip the local directory, excluding large directories from the zip. You may want to keep them.\\nzip -r local_changes.zip . -x  \".git/*\" \"checkpoints/*\" \"data/*\" \"out/*\"\\n# Copy the .zip file to the TPU VM\\ngcloud compute tpus tpu-vm scp --worker=all local_changes.zip \"litgpt:~\"\\n# Unzip on each host\\ngcloud compute tpus tpu-vm ssh litgpt --worker=all --command=\"cd ~; unzip -q -o local_changes.zip\"\\n\\n# Example of a typical workflow\\ngcloud compute tpus tpu-vm ssh tmp --worker=all --command=\"cd ~; bash install_dependencies.sh\"\\ngcloud compute tpus tpu-vm ssh tmp --worker=all --command=\"cd ~; bash prepare_checkpoints.sh\"\\ngcloud compute tpus tpu-vm ssh tmp --worker=all --command=\"cd ~; bash run_desired_script.sh\"</td>\n",
       "      <td>How does this project support TPUs?\\n</td>\n",
       "      <td>This project supports TPUs via PyTorch XLA, which is integrated into Fabric.</td>\n",
       "      <td>extensions/xla/README.md</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>| Config                            | Model                  | Epochs | Max seq length | Micro batch size | Machine | Training runtime | Cost | Peak memory | Validation loss | Validation perplexity | Multitask score (MMLU) |\\n| --------------------------------- | ---------------------- | ------ | -------------- | ---------------- | ------- | ---------------- | ---- | ----------- | --------------- | --------------------- | --------------- |\\n| falcon-7b/lora.yaml               | falcon-7b              | 4      | 512            | 1                | 1xA10G  | 24.84 min        | $0.7 | 16.69 GB    | 0.945           | 2.573                 | 26.2%           |\\n| falcon-7b/lora.yaml               | falcon-7b              | 4      | 512            | 1                | 4xA10G  | 24.94 min        | $2.0 | 16.69 GB    | 0.945           | 2.573                 | 26.4%           |\\n| falcon-7b/qlora.yaml              | falcon-7b              | 4      | 512            | 1                | 1xA10G  | 50.85 min        | $1.5 | 9.44 GB     | 0.993           | 2.699                 | 26.3%           |\\n| falcon-7b/qlora.yaml              | falcon-7b              | 4      | 512            | 1                | 4xA10G  | 50.88 min        | $4.1 | 9.44 GB     | 0.993           | 2.699                 | 26.3%           |\\n|                                   |                        |        |                |                  |         |                  |      |             |                 |                       |                 |\\n| gemma-2b/full.yaml                | gemma-2b               | 1      | 512            | 1                | 4xA10G  | 14.06 min        | $1.1 | 17.43 GB    | 1.021           | 2.777                 | 32.4%           |\\n| gemma-2b/lora.yaml                | gemma-2b               | 2      | 512            | 2                | 1xA10G  | 9.41 min         | $0.3 | 12.62 GB    | 0.981           | 2.666                 | 34.4%           |</td>\n",
       "      <td>What is the training runtime for the gemma-2b model with the lora configuration?\\n</td>\n",
       "      <td>9.41 min</td>\n",
       "      <td>config_hub/finetune/README.md</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>| Size  | Model          | Quantization | GPU      | Max GPU RAM                               | Token/sec |\\n|-------|----------------|--------------|----------|-------------------------------------------|-----------|\\n| 1.3 B | phi-1.5        | None         | 1 x A100 | 2.86 GB                                   | 42.56     |\\n| 1.3 B | phi-1.5        | bnb.nf4      | 1 x A100 | 1.39 GB                                   | 22.89     |\\n| 1.3 B | phi-1.5        | bnb.nf4-dq   | 1 x A100 | 1.33 GB                                   | 22.75     |\\n|       |                |              |          |                                           |           |\\n| 3 B   | StableLM Alpha | None         | 1 x A100 | 7.30 GB                                   | 49.01     |\\n| 3 B   | StableLM Alpha | bnb.nf4      | 1 x A100 | 3.20 GB                                   | 29.04     |\\n| 3 B   | StableLM Alpha | bnb.nf4-dq   | 1 x A100 | 3.04 GB                                   | 27.15     |\\n|       |                |              |          |                                           |           |\\n| 7 B   | Llama 2        | None         | 1 x A100 | 13.52 GB                                  | 30.97     |\\n| 7 B   | Llama 2        | bnb.nf4      | 1 x A100 | 4.57 GB                                   | 19.98     |\\n| 7 B   | Llama 2        | bnb.nf4-dq   | 1 x A100 | 4.26 GB                                   | 17.3      |\\n|       |                |              |          |                                           |           |\\n| 13 B  | Llama 2        | None         | 1 x A100 | 26.21 GB                                  | 24.82     |\\n| 13 B  | Llama 2        | bnb.nf4      | 1 x A100 | 8.32 GB                                   | 16.73     |\\n| 13 B  | Llama 2        | bnb.nf4-dq   | 1 x A100 | 7.72 GB                                   | 14.43     |\\n|       |                |              |          |                                           |           |</td>\n",
       "      <td>What is the maximum GPU RAM required for the 13 B Llama 2 model with bnb.nf4-dq quantization?\\n</td>\n",
       "      <td>7.72 GB</td>\n",
       "      <td>tutorials/resource-tables.md</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td># Download Model Weights with LitGPT\\n\\nLitGPT supports a variety of LLM architectures with publicly available weights. You can download model weights and access a list of supported models using the LitGPT `download.py` script.</td>\n",
       "      <td>What can I use to download model weights in LitGPT?\\n</td>\n",
       "      <td>The LitGPT `download.py` script</td>\n",
       "      <td>tutorials/download_model_weights.md</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td># Serve and Deploy LLMs\\n\\nThis document shows how you can serve a LitGPT for deployment. \\n\\n&amp;nbsp;\\n## Serve an LLM\\n\\nThis section illustrates how we can set up an inference server for a phi-2 LLM using `litgpt serve` that is minimal and highly scalable.\\n\\n\\n&amp;nbsp;\\n## Step 1: Start the inference server\\n\\n\\n```bash\\n# 1) Download a pretrained model (alternatively, use your own finetuned model)\\nlitgpt download --repo_id microsoft/phi-2\\n\\n# 2) Start the server\\nlitgpt serve --checkpoint_dir checkpoints/microsoft/phi-2\\n```\\n\\n&gt; [!TIP]\\n&gt; Use `litgpt serve --help` to display additional options, including the port, devices, LLM temperature setting, and more.\\n\\n\\n&amp;nbsp;\\n## Step 2: Query the inference server\\n\\nYou can now send requests to the inference server you started in step 2. For example, in a new Python session, we can send requests to the inference server as follows:\\n\\n\\n```python\\nimport requests, json\\n\\nresponse = requests.post(\\n    \"http://127.0.0.1:8000/predict\", \\n    json={\"prompt\": \"Fix typos in the following sentence: Exampel input\"}\\n)\\n\\nprint(response.json()[\"output\"])\\n```\\n\\nExecuting the code above prints the following output:\\n\\n```\\nInstruct: Fix typos in the following sentence: Exampel input\\nOutput: Example input.\\n```</td>\n",
       "      <td>How can I start an inference server for a phi-2 LLM using litgpt serve?\\n</td>\n",
       "      <td>You can start an inference server for a phi-2 LLM using litgpt serve by first downloading a pretrained model using `litgpt download --repo_id microsoft/phi-2` and then starting the server using `litgpt serve --checkpoint_dir checkpoints/microsoft/phi-2`.</td>\n",
       "      <td>tutorials/deploy.md</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         context  \\\n",
       "105  # TPU support\\n\\nThis project utilizes [`Fabric`](https://lightning.ai/docs/fabric/stable), which supports TPUs via [PyTorch XLA](https://github.com/pytorch/xla).\\n\\n> [!NOTE]\\n> This guide assumes that you have already set-up your [Google Cloud environment](https://cloud.google.com/run/docs/setup).\\n\\nTo set up a Google Cloud instance with a TPU v4 VM, run the following commands:\\n\\n```shell\\ngcloud compute tpus tpu-vm create litgpt --version=tpu-vm-v4-base --accelerator-type=v4-8 --zone=us-central2-b\\ngcloud compute tpus tpu-vm ssh litgpt --zone=us-central2-b\\n```\\n\\nYou can also choose a different TPU type. To do so, change the `version`, `accelerator-type`, and `zone` arguments. Find all regions and zones [here](https://cloud.google.com/tpu/docs/regions-zones).\\n\\n<details>\\n<summary>Multihost caveats</summary>\\n\\nTPU v4-8 uses a single host. SSH'ing into the machine and running commands manually will only work when using a single host (1 slice in the TPU pod).\\nIn multi-host environments, such as larger TPU pod slices, it's necessary to launch all commands on all hosts simultaneously to avoid hangs.\\nFor local development, it is advisable to upload a zip file containing all your current changes and execute it inside the VM from your personal computer:\\n\\n```shell\\n# Zip the local directory, excluding large directories from the zip. You may want to keep them.\\nzip -r local_changes.zip . -x  \".git/*\" \"checkpoints/*\" \"data/*\" \"out/*\"\\n# Copy the .zip file to the TPU VM\\ngcloud compute tpus tpu-vm scp --worker=all local_changes.zip \"litgpt:~\"\\n# Unzip on each host\\ngcloud compute tpus tpu-vm ssh litgpt --worker=all --command=\"cd ~; unzip -q -o local_changes.zip\"\\n\\n# Example of a typical workflow\\ngcloud compute tpus tpu-vm ssh tmp --worker=all --command=\"cd ~; bash install_dependencies.sh\"\\ngcloud compute tpus tpu-vm ssh tmp --worker=all --command=\"cd ~; bash prepare_checkpoints.sh\"\\ngcloud compute tpus tpu-vm ssh tmp --worker=all --command=\"cd ~; bash run_desired_script.sh\"   \n",
       "125                                     | Config                            | Model                  | Epochs | Max seq length | Micro batch size | Machine | Training runtime | Cost | Peak memory | Validation loss | Validation perplexity | Multitask score (MMLU) |\\n| --------------------------------- | ---------------------- | ------ | -------------- | ---------------- | ------- | ---------------- | ---- | ----------- | --------------- | --------------------- | --------------- |\\n| falcon-7b/lora.yaml               | falcon-7b              | 4      | 512            | 1                | 1xA10G  | 24.84 min        | $0.7 | 16.69 GB    | 0.945           | 2.573                 | 26.2%           |\\n| falcon-7b/lora.yaml               | falcon-7b              | 4      | 512            | 1                | 4xA10G  | 24.94 min        | $2.0 | 16.69 GB    | 0.945           | 2.573                 | 26.4%           |\\n| falcon-7b/qlora.yaml              | falcon-7b              | 4      | 512            | 1                | 1xA10G  | 50.85 min        | $1.5 | 9.44 GB     | 0.993           | 2.699                 | 26.3%           |\\n| falcon-7b/qlora.yaml              | falcon-7b              | 4      | 512            | 1                | 4xA10G  | 50.88 min        | $4.1 | 9.44 GB     | 0.993           | 2.699                 | 26.3%           |\\n|                                   |                        |        |                |                  |         |                  |      |             |                 |                       |                 |\\n| gemma-2b/full.yaml                | gemma-2b               | 1      | 512            | 1                | 4xA10G  | 14.06 min        | $1.1 | 17.43 GB    | 1.021           | 2.777                 | 32.4%           |\\n| gemma-2b/lora.yaml                | gemma-2b               | 2      | 512            | 2                | 1xA10G  | 9.41 min         | $0.3 | 12.62 GB    | 0.981           | 2.666                 | 34.4%           |   \n",
       "121                                   | Size  | Model          | Quantization | GPU      | Max GPU RAM                               | Token/sec |\\n|-------|----------------|--------------|----------|-------------------------------------------|-----------|\\n| 1.3 B | phi-1.5        | None         | 1 x A100 | 2.86 GB                                   | 42.56     |\\n| 1.3 B | phi-1.5        | bnb.nf4      | 1 x A100 | 1.39 GB                                   | 22.89     |\\n| 1.3 B | phi-1.5        | bnb.nf4-dq   | 1 x A100 | 1.33 GB                                   | 22.75     |\\n|       |                |              |          |                                           |           |\\n| 3 B   | StableLM Alpha | None         | 1 x A100 | 7.30 GB                                   | 49.01     |\\n| 3 B   | StableLM Alpha | bnb.nf4      | 1 x A100 | 3.20 GB                                   | 29.04     |\\n| 3 B   | StableLM Alpha | bnb.nf4-dq   | 1 x A100 | 3.04 GB                                   | 27.15     |\\n|       |                |              |          |                                           |           |\\n| 7 B   | Llama 2        | None         | 1 x A100 | 13.52 GB                                  | 30.97     |\\n| 7 B   | Llama 2        | bnb.nf4      | 1 x A100 | 4.57 GB                                   | 19.98     |\\n| 7 B   | Llama 2        | bnb.nf4-dq   | 1 x A100 | 4.26 GB                                   | 17.3      |\\n|       |                |              |          |                                           |           |\\n| 13 B  | Llama 2        | None         | 1 x A100 | 26.21 GB                                  | 24.82     |\\n| 13 B  | Llama 2        | bnb.nf4      | 1 x A100 | 8.32 GB                                   | 16.73     |\\n| 13 B  | Llama 2        | bnb.nf4-dq   | 1 x A100 | 7.72 GB                                   | 14.43     |\\n|       |                |              |          |                                           |           |   \n",
       "87                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           # Download Model Weights with LitGPT\\n\\nLitGPT supports a variety of LLM architectures with publicly available weights. You can download model weights and access a list of supported models using the LitGPT `download.py` script.   \n",
       "104                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      # Serve and Deploy LLMs\\n\\nThis document shows how you can serve a LitGPT for deployment. \\n\\n&nbsp;\\n## Serve an LLM\\n\\nThis section illustrates how we can set up an inference server for a phi-2 LLM using `litgpt serve` that is minimal and highly scalable.\\n\\n\\n&nbsp;\\n## Step 1: Start the inference server\\n\\n\\n```bash\\n# 1) Download a pretrained model (alternatively, use your own finetuned model)\\nlitgpt download --repo_id microsoft/phi-2\\n\\n# 2) Start the server\\nlitgpt serve --checkpoint_dir checkpoints/microsoft/phi-2\\n```\\n\\n> [!TIP]\\n> Use `litgpt serve --help` to display additional options, including the port, devices, LLM temperature setting, and more.\\n\\n\\n&nbsp;\\n## Step 2: Query the inference server\\n\\nYou can now send requests to the inference server you started in step 2. For example, in a new Python session, we can send requests to the inference server as follows:\\n\\n\\n```python\\nimport requests, json\\n\\nresponse = requests.post(\\n    \"http://127.0.0.1:8000/predict\", \\n    json={\"prompt\": \"Fix typos in the following sentence: Exampel input\"}\\n)\\n\\nprint(response.json()[\"output\"])\\n```\\n\\nExecuting the code above prints the following output:\\n\\n```\\nInstruct: Fix typos in the following sentence: Exampel input\\nOutput: Example input.\\n```   \n",
       "\n",
       "                                                                                            question  \\\n",
       "105                                                            How does this project support TPUs?\\n   \n",
       "125               What is the training runtime for the gemma-2b model with the lora configuration?\\n   \n",
       "121  What is the maximum GPU RAM required for the 13 B Llama 2 model with bnb.nf4-dq quantization?\\n   \n",
       "87                                             What can I use to download model weights in LitGPT?\\n   \n",
       "104                        How can I start an inference server for a phi-2 LLM using litgpt serve?\\n   \n",
       "\n",
       "                                                                                                                                                                                                                                                             answer  \\\n",
       "105                                                                                                                                                                                    This project supports TPUs via PyTorch XLA, which is integrated into Fabric.   \n",
       "125                                                                                                                                                                                                                                                        9.41 min   \n",
       "121                                                                                                                                                                                                                                                         7.72 GB   \n",
       "87                                                                                                                                                                                                                                  The LitGPT `download.py` script   \n",
       "104  You can start an inference server for a phi-2 LLM using litgpt serve by first downloading a pretrained model using `litgpt download --repo_id microsoft/phi-2` and then starting the server using `litgpt serve --checkpoint_dir checkpoints/microsoft/phi-2`.   \n",
       "\n",
       "                              source_doc  \n",
       "105             extensions/xla/README.md  \n",
       "125        config_hub/finetune/README.md  \n",
       "121         tutorials/resource-tables.md  \n",
       "87   tutorials/download_model_weights.md  \n",
       "104                  tutorials/deploy.md  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(pd.DataFrame(outputs).sample(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set-up Critique Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_groundedness_critique_prompt = \"\"\"\n",
    "You will be given a context and a question.\n",
    "Your task is to provide a 'total rating' scoring how well one can answer the given question unambiguously with the given context.\n",
    "Give your answer on a scale of 1 to 5, where 1 means that the question is not answerable at all given the context, and 5 means that the question is clearly and unambiguously answerable with the context.\n",
    "\n",
    "Provide your answer as follows:\n",
    "\n",
    "Answer:::\n",
    "Evaluation: (your rationale for the rating, as a text)\n",
    "Total rating: (your rating, as a number between 1 and 5)\n",
    "\n",
    "You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
    "\n",
    "Now here are the question and context.\n",
    "\n",
    "Question: {question}\\n\n",
    "Context: {context}\\n\n",
    "Answer::: \"\"\"\n",
    "\n",
    "question_relevance_critique_prompt = \"\"\"\n",
    "You will be given a question.\n",
    "Your task is to provide a 'total rating' representing how useful this question can be to AI and ML Practitioners working with Large Language Models using litgpt.\n",
    "Give your answer on a scale of 1 to 5, where 1 means that the question is not useful at all, and 5 means that the question is extremely useful.\n",
    "\n",
    "Provide your answer as follows:\n",
    "\n",
    "Answer:::\n",
    "Evaluation: (your rationale for the rating, as a text)\n",
    "Total rating: (your rating, as a number between 1 and 5)\n",
    "\n",
    "You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
    "\n",
    "Now here is the question.\n",
    "\n",
    "Question: {question}\\n\n",
    "Answer::: \"\"\"\n",
    "\n",
    "question_standalone_critique_prompt = \"\"\"\n",
    "You will be given a question.\n",
    "Your task is to provide a 'total rating' representing how context-independent this question is.\n",
    "Give your answer on a scale of 1 to 5, where 1 means that the question depends on additional information to be understood, and 5 means that the question makes sense by itself.\n",
    "For instance, if the question refers to a particular setting, like 'in the context' or 'in the document', the rating must be 1.\n",
    "The questions can contain specific technical nouns or acronyms like LoRA, fp4, litgpt or Llama 3 and still be a 5: it must simply be clear to an operator with access to documentation what the question is about.\n",
    "\n",
    "Provide your answer as follows:\n",
    "\n",
    "Answer:::\n",
    "Evaluation: (your rationale for the rating, as a text)\n",
    "Total rating: (your rating, as a number between 1 and 5)\n",
    "\n",
    "You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
    "\n",
    "Now here is the question.\n",
    "\n",
    "Question: {question}\\n\n",
    "Answer::: \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating critique for each QA couple...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 127/127 [1:28:56<00:00, 42.02s/it]\n"
     ]
    }
   ],
   "source": [
    "print(\"Generating critique for each QA couple...\")\n",
    "for output in tqdm(outputs):\n",
    "    evaluations = {\n",
    "        \"groundedness\": call_llm(\n",
    "            llm_client,\n",
    "            question_groundedness_critique_prompt.format(\n",
    "                context=output[\"context\"], question=output[\"question\"]\n",
    "            ),\n",
    "        ),\n",
    "        \"relevance\": call_llm(\n",
    "            llm_client,\n",
    "            question_relevance_critique_prompt.format(question=output[\"question\"]),\n",
    "        ),\n",
    "        \"standalone\": call_llm(\n",
    "            llm_client,\n",
    "            question_standalone_critique_prompt.format(question=output[\"question\"]),\n",
    "        ),\n",
    "    }\n",
    "    try:\n",
    "        for criterion, evaluation in evaluations.items():\n",
    "            score, eval = (\n",
    "                int(evaluation.split(\"Total rating: \")[-1].strip()),\n",
    "                evaluation.split(\"Total rating: \")[-2].split(\"Evaluation: \")[1],\n",
    "            )\n",
    "            output.update(\n",
    "                {\n",
    "                    f\"{criterion}_score\": score,\n",
    "                    f\"{criterion}_eval\": eval,\n",
    "                }\n",
    "            )\n",
    "    except Exception as e:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation dataset before filtering:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>groundedness_score</th>\n",
       "      <th>relevance_score</th>\n",
       "      <th>standalone_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the shape of tensor t24?\\n</td>\n",
       "      <td>The shape of tensor t24 is [2, 5, 4096].\\n```</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is a config in LitGPT?\\n</td>\n",
       "      <td>In LitGPT, a config is a configuration file that lets you customize training for all granular parameters like learning rate, batch size, number of epochs, and more.</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How many parameters are there in the LLaMA-Adapter v2?\\n</td>\n",
       "      <td>There are ~2.3 M trainable parameters in the LLaMA-Adapter v2.</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is the name of the argument used to resume training?\\n</td>\n",
       "      <td>The name of the argument used to resume training is `--resume`.</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What was the time taken to complete training?\\n</td>\n",
       "      <td>The time taken to complete training was ~ 4 weeks with 64 A100 GPUs.</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>What is the URL for the Lightning Studio templates?\\n</td>\n",
       "      <td>https://lightning.ai/lightning-ai/studios</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>What is the version of nvfuser\\_cu121 used?\\n</td>\n",
       "      <td>The version of nvfuser\\_cu121 used is 0.2.0.dev20240327.</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>What is the command to download the pretrained model weights for the Llama-2-7b-hf model?\\n</td>\n",
       "      <td>`litgpt download --repo_id meta-llama/Llama-2-7b-hf`</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>What is the training runtime for the gemma-2b model with the lora configuration?\\n</td>\n",
       "      <td>9.41 min</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>What is the original size of the Alpaca dataset?\\n</td>\n",
       "      <td>The original size of the Alpaca dataset is 52,000 instructions and demonstrations.</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>127 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                        question  \\\n",
       "0                                                             What is the shape of tensor t24?\\n   \n",
       "1                                                                  What is a config in LitGPT?\\n   \n",
       "2                                       How many parameters are there in the LLaMA-Adapter v2?\\n   \n",
       "3                                    What is the name of the argument used to resume training?\\n   \n",
       "4                                                What was the time taken to complete training?\\n   \n",
       "..                                                                                           ...   \n",
       "122                                        What is the URL for the Lightning Studio templates?\\n   \n",
       "123                                                What is the version of nvfuser\\_cu121 used?\\n   \n",
       "124  What is the command to download the pretrained model weights for the Llama-2-7b-hf model?\\n   \n",
       "125           What is the training runtime for the gemma-2b model with the lora configuration?\\n   \n",
       "126                                           What is the original size of the Alpaca dataset?\\n   \n",
       "\n",
       "                                                                                                                                                                   answer  \\\n",
       "0                                                                                                                           The shape of tensor t24 is [2, 5, 4096].\\n```   \n",
       "1    In LitGPT, a config is a configuration file that lets you customize training for all granular parameters like learning rate, batch size, number of epochs, and more.   \n",
       "2                                                                                                          There are ~2.3 M trainable parameters in the LLaMA-Adapter v2.   \n",
       "3                                                                                                         The name of the argument used to resume training is `--resume`.   \n",
       "4                                                                                                    The time taken to complete training was ~ 4 weeks with 64 A100 GPUs.   \n",
       "..                                                                                                                                                                    ...   \n",
       "122                                                                                                                             https://lightning.ai/lightning-ai/studios   \n",
       "123                                                                                                              The version of nvfuser\\_cu121 used is 0.2.0.dev20240327.   \n",
       "124                                                                                                                  `litgpt download --repo_id meta-llama/Llama-2-7b-hf`   \n",
       "125                                                                                                                                                              9.41 min   \n",
       "126                                                                                    The original size of the Alpaca dataset is 52,000 instructions and demonstrations.   \n",
       "\n",
       "     groundedness_score  relevance_score  standalone_score  \n",
       "0                   NaN              NaN               NaN  \n",
       "1                   5.0              NaN               NaN  \n",
       "2                   2.0              NaN               NaN  \n",
       "3                   1.0              5.0               3.0  \n",
       "4                   5.0              NaN               NaN  \n",
       "..                  ...              ...               ...  \n",
       "122                 2.0              NaN               NaN  \n",
       "123                 4.0              3.0               4.0  \n",
       "124                 3.0              5.0               5.0  \n",
       "125                 3.0              3.0               5.0  \n",
       "126                 1.0              5.0               5.0  \n",
       "\n",
       "[127 rows x 5 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================\n",
      "Final evaluation dataset:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>groundedness_score</th>\n",
       "      <th>relevance_score</th>\n",
       "      <th>standalone_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the memory usage of Llama 2 with 7B when using bnb.nf4-dq?\\n</td>\n",
       "      <td>13.84 GB</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the command to run the evaluation harness?\\n</td>\n",
       "      <td>The command to run the evaluation harness is `lm_eval --model hf --model_args pretrained=out/hf-tinyllama/converted --tasks \"hellaswag,gsm8k,truthfulqa_mc2,mmlu,winogrande,arc_challenge\" --device \"cuda:0\" --batch_size 4`.</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is the command to run the Evaluation Harness?\\n</td>\n",
       "      <td>The command to run the Evaluation Harness is `lm_eval --model hf --model_args pretrained=\"out/converted_model\" --tasks \"hellaswag,gsm8k,truthfulqa_mc2,mmlu,winogrande,arc_challenge\" --device \"cuda:0\" --batch_size 4`.</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is the default value of the 'precision' parameter in the LoRA finetuning config?\\n</td>\n",
       "      <td>The default value of the 'precision' parameter in the LoRA finetuning config is null.</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is the name of the directory where the model weights are stored by default?\\n</td>\n",
       "      <td>The model weights are stored in a `./checkpoints` subdirectory by default.</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>What is the command to download a pretrained model?\\n</td>\n",
       "      <td>litgpt download --repo_id [model_name]</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>What is the name of the studio that provides LitGPT pretraining projects?\\n</td>\n",
       "      <td>Lightning Studio</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>How long does it take to finetune a model on a GPU?\\n</td>\n",
       "      <td>It takes about a minute to finetune a model on a GPU.</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>What is the most memory-intensive finetuning technique in LitGPT?\\n</td>\n",
       "      <td>The most memory-intensive finetuning technique in LitGPT is full finetuning.</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>What is the recommended approach for preprocessing large datasets?\\n</td>\n",
       "      <td>The recommended approach for preprocessing large datasets is to use LitData for preprocessing and then read it from a local directory or S3 connection using `--data LitData`.</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>How many maximum steps are set for the quick test run in the example?\\n</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>How to evaluate a LoRA-finetuned LLM?\\n</td>\n",
       "      <td>You can evaluate a LoRA-finetuned LLM by running `litgpt evaluate` with the `--checkpoint_dir` pointing to the directory containing the finetuned model files. No further conversion is necessary as the `finetune lora` command already prepares the necessary merged model files.</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>What is the recommended micro batch size for finetuning Adapter V2 on a GPU with 12GB memory?\\n</td>\n",
       "      <td>To fit Adapter V2 to 12GB memory set `--train.micro_batch_size 2`.</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>How can I get started with using LitGPT?\\n</td>\n",
       "      <td>You can start with the \"Zero to LitGPT: Getting Started with Pretraining, Finetuning, and Using LLMs\" tutorial.</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>What is the name of the model developed by Microsoft Research with 1.3B parameters?\\n</td>\n",
       "      <td>Phi</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>How do I chat with the phi-2 model from Microsoft?\\n</td>\n",
       "      <td>You can chat with the phi-2 model from Microsoft by downloading it using the `litgpt download --repo_id microsoft/phi-2` command and then running the `litgpt chat --checkpoint_dir checkpoints/microsoft/phi-2` command.</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>What is TinyLlama's architecture similar to?\\n</td>\n",
       "      <td>TinyLlama's architecture is similar to Meta AI's LLama 2.</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>What can I use to download model weights in LitGPT?\\n</td>\n",
       "      <td>The LitGPT `download.py` script</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>How many samples are in the Alpaca dataset?\\n</td>\n",
       "      <td>The Alpaca dataset contains 51,759 samples.</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>What is the command to download a pretrained model in litgpt?\\n</td>\n",
       "      <td>`litgpt download --repo_id EleutherAI/pythia-160m`</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>How much disk space is required to store the SlimPajama dataset?\\n</td>\n",
       "      <td>2.5 TB of disk space is required to store the SlimPajama dataset.</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>How can I start an inference server for a phi-2 LLM using litgpt serve?\\n</td>\n",
       "      <td>You can start an inference server for a phi-2 LLM using litgpt serve by first downloading a pretrained model using `litgpt download --repo_id microsoft/phi-2` and then starting the server using `litgpt serve --checkpoint_dir checkpoints/microsoft/phi-2`.</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>How does this project support TPUs?\\n</td>\n",
       "      <td>This project supports TPUs via PyTorch XLA, which is integrated into Fabric.</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>How do you merge LoRA weights into the original model's checkpoint?\\n</td>\n",
       "      <td>You can merge LoRA weights into the original model's checkpoint using the `litgpt merge_lora` command followed by the `--checkpoint_dir` flag and the path to the checkpoint directory. This creates a full `lit_model.pth` checkpoint file.</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>What is the maximum GPU RAM used by the 1.3B phi-1.5 model with quantization None and microbatch size 4?\\n</td>\n",
       "      <td>10.68 GB</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>What command do I use to convert the finetuned model to a HF transformer model?\\n</td>\n",
       "      <td>You convert the finetuned model to a HF transformer model by using the `litgpt convert from_litgpt` command followed by the specified `checkpoint_dir` and `output_dir`.</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Which model has 1.3 billion parameters?\\n</td>\n",
       "      <td>Phi by Microsoft Research</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>What is the version of nvfuser\\_cu121 used?\\n</td>\n",
       "      <td>The version of nvfuser\\_cu121 used is 0.2.0.dev20240327.</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>What is the command to download the pretrained model weights for the Llama-2-7b-hf model?\\n</td>\n",
       "      <td>`litgpt download --repo_id meta-llama/Llama-2-7b-hf`</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>What is the training runtime for the gemma-2b model with the lora configuration?\\n</td>\n",
       "      <td>9.41 min</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                      question  \\\n",
       "0                                         What is the memory usage of Llama 2 with 7B when using bnb.nf4-dq?\\n   \n",
       "1                                                         What is the command to run the evaluation harness?\\n   \n",
       "2                                                         What is the command to run the Evaluation Harness?\\n   \n",
       "3                      What is the default value of the 'precision' parameter in the LoRA finetuning config?\\n   \n",
       "4                           What is the name of the directory where the model weights are stored by default?\\n   \n",
       "5                                                        What is the command to download a pretrained model?\\n   \n",
       "6                                  What is the name of the studio that provides LitGPT pretraining projects?\\n   \n",
       "7                                                        How long does it take to finetune a model on a GPU?\\n   \n",
       "8                                          What is the most memory-intensive finetuning technique in LitGPT?\\n   \n",
       "9                                         What is the recommended approach for preprocessing large datasets?\\n   \n",
       "10                                     How many maximum steps are set for the quick test run in the example?\\n   \n",
       "11                                                                     How to evaluate a LoRA-finetuned LLM?\\n   \n",
       "12             What is the recommended micro batch size for finetuning Adapter V2 on a GPU with 12GB memory?\\n   \n",
       "13                                                                  How can I get started with using LitGPT?\\n   \n",
       "14                       What is the name of the model developed by Microsoft Research with 1.3B parameters?\\n   \n",
       "15                                                        How do I chat with the phi-2 model from Microsoft?\\n   \n",
       "16                                                              What is TinyLlama's architecture similar to?\\n   \n",
       "17                                                       What can I use to download model weights in LitGPT?\\n   \n",
       "18                                                               How many samples are in the Alpaca dataset?\\n   \n",
       "19                                             What is the command to download a pretrained model in litgpt?\\n   \n",
       "20                                          How much disk space is required to store the SlimPajama dataset?\\n   \n",
       "21                                   How can I start an inference server for a phi-2 LLM using litgpt serve?\\n   \n",
       "22                                                                       How does this project support TPUs?\\n   \n",
       "23                                       How do you merge LoRA weights into the original model's checkpoint?\\n   \n",
       "24  What is the maximum GPU RAM used by the 1.3B phi-1.5 model with quantization None and microbatch size 4?\\n   \n",
       "25                           What command do I use to convert the finetuned model to a HF transformer model?\\n   \n",
       "26                                                                   Which model has 1.3 billion parameters?\\n   \n",
       "27                                                               What is the version of nvfuser\\_cu121 used?\\n   \n",
       "28                 What is the command to download the pretrained model weights for the Llama-2-7b-hf model?\\n   \n",
       "29                          What is the training runtime for the gemma-2b model with the lora configuration?\\n   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                 answer  \\\n",
       "0                                                                                                                                                                                                                                                                              13.84 GB   \n",
       "1                                                         The command to run the evaluation harness is `lm_eval --model hf --model_args pretrained=out/hf-tinyllama/converted --tasks \"hellaswag,gsm8k,truthfulqa_mc2,mmlu,winogrande,arc_challenge\" --device \"cuda:0\" --batch_size 4`.   \n",
       "2                                                              The command to run the Evaluation Harness is `lm_eval --model hf --model_args pretrained=\"out/converted_model\" --tasks \"hellaswag,gsm8k,truthfulqa_mc2,mmlu,winogrande,arc_challenge\" --device \"cuda:0\" --batch_size 4`.   \n",
       "3                                                                                                                                                                                                 The default value of the 'precision' parameter in the LoRA finetuning config is null.   \n",
       "4                                                                                                                                                                                                            The model weights are stored in a `./checkpoints` subdirectory by default.   \n",
       "5                                                                                                                                                                                                                                                litgpt download --repo_id [model_name]   \n",
       "6                                                                                                                                                                                                                                                                      Lightning Studio   \n",
       "7                                                                                                                                                                                                                                 It takes about a minute to finetune a model on a GPU.   \n",
       "8                                                                                                                                                                                                          The most memory-intensive finetuning technique in LitGPT is full finetuning.   \n",
       "9                                                                                                        The recommended approach for preprocessing large datasets is to use LitData for preprocessing and then read it from a local directory or S3 connection using `--data LitData`.   \n",
       "10                                                                                                                                                                                                                                                                                    5   \n",
       "11  You can evaluate a LoRA-finetuned LLM by running `litgpt evaluate` with the `--checkpoint_dir` pointing to the directory containing the finetuned model files. No further conversion is necessary as the `finetune lora` command already prepares the necessary merged model files.   \n",
       "12                                                                                                                                                                                                                   To fit Adapter V2 to 12GB memory set `--train.micro_batch_size 2`.   \n",
       "13                                                                                                                                                                      You can start with the \"Zero to LitGPT: Getting Started with Pretraining, Finetuning, and Using LLMs\" tutorial.   \n",
       "14                                                                                                                                                                                                                                                                                  Phi   \n",
       "15                                                            You can chat with the phi-2 model from Microsoft by downloading it using the `litgpt download --repo_id microsoft/phi-2` command and then running the `litgpt chat --checkpoint_dir checkpoints/microsoft/phi-2` command.   \n",
       "16                                                                                                                                                                                                                            TinyLlama's architecture is similar to Meta AI's LLama 2.   \n",
       "17                                                                                                                                                                                                                                                      The LitGPT `download.py` script   \n",
       "18                                                                                                                                                                                                                                          The Alpaca dataset contains 51,759 samples.   \n",
       "19                                                                                                                                                                                                                                   `litgpt download --repo_id EleutherAI/pythia-160m`   \n",
       "20                                                                                                                                                                                                                    2.5 TB of disk space is required to store the SlimPajama dataset.   \n",
       "21                       You can start an inference server for a phi-2 LLM using litgpt serve by first downloading a pretrained model using `litgpt download --repo_id microsoft/phi-2` and then starting the server using `litgpt serve --checkpoint_dir checkpoints/microsoft/phi-2`.   \n",
       "22                                                                                                                                                                                                         This project supports TPUs via PyTorch XLA, which is integrated into Fabric.   \n",
       "23                                         You can merge LoRA weights into the original model's checkpoint using the `litgpt merge_lora` command followed by the `--checkpoint_dir` flag and the path to the checkpoint directory. This creates a full `lit_model.pth` checkpoint file.   \n",
       "24                                                                                                                                                                                                                                                                             10.68 GB   \n",
       "25                                                                                                             You convert the finetuned model to a HF transformer model by using the `litgpt convert from_litgpt` command followed by the specified `checkpoint_dir` and `output_dir`.   \n",
       "26                                                                                                                                                                                                                                                            Phi by Microsoft Research   \n",
       "27                                                                                                                                                                                                                             The version of nvfuser\\_cu121 used is 0.2.0.dev20240327.   \n",
       "28                                                                                                                                                                                                                                 `litgpt download --repo_id meta-llama/Llama-2-7b-hf`   \n",
       "29                                                                                                                                                                                                                                                                             9.41 min   \n",
       "\n",
       "    groundedness_score  relevance_score  standalone_score  \n",
       "0                  5.0              3.0               3.0  \n",
       "1                  5.0              5.0               5.0  \n",
       "2                  5.0              5.0               4.0  \n",
       "3                  5.0              4.0               5.0  \n",
       "4                  5.0              5.0               5.0  \n",
       "5                  5.0              4.0               5.0  \n",
       "6                  5.0              4.0               5.0  \n",
       "7                  3.0              4.0               4.0  \n",
       "8                  5.0              4.0               5.0  \n",
       "9                  3.0              5.0               5.0  \n",
       "10                 5.0              4.0               4.0  \n",
       "11                 5.0              5.0               5.0  \n",
       "12                 5.0              5.0               5.0  \n",
       "13                 3.0              5.0               5.0  \n",
       "14                 5.0              4.0               5.0  \n",
       "15                 5.0              4.0               5.0  \n",
       "16                 5.0              4.0               5.0  \n",
       "17                 4.0              4.0               5.0  \n",
       "18                 5.0              5.0               5.0  \n",
       "19                 5.0              5.0               5.0  \n",
       "20                 3.0              5.0               5.0  \n",
       "21                 5.0              5.0               5.0  \n",
       "22                 5.0              5.0               5.0  \n",
       "23                 5.0              5.0               5.0  \n",
       "24                 5.0              3.0               5.0  \n",
       "25                 3.0              5.0               5.0  \n",
       "26                 5.0              4.0               4.0  \n",
       "27                 4.0              3.0               4.0  \n",
       "28                 3.0              5.0               5.0  \n",
       "29                 3.0              3.0               5.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Filter out the bad questions, keep 3 as the threshold for now\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "\n",
    "generated_questions = pd.DataFrame.from_dict(outputs)\n",
    "\n",
    "# Filter out low rated QA pairs\n",
    "\n",
    "print(\"Evaluation dataset before filtering:\")\n",
    "display(\n",
    "    generated_questions[\n",
    "        [\n",
    "            \"question\",\n",
    "            \"answer\",\n",
    "            \"groundedness_score\",\n",
    "            \"relevance_score\",\n",
    "            \"standalone_score\",\n",
    "        ]\n",
    "    ]\n",
    ")\n",
    "generated_questions = generated_questions.loc[\n",
    "    (generated_questions[\"groundedness_score\"] >= 3)\n",
    "    & (generated_questions[\"relevance_score\"] >= 3)\n",
    "    & (generated_questions[\"standalone_score\"] >= 3)\n",
    "]\n",
    "print(\"============================================\")\n",
    "print(\"Final evaluation dataset:\")\n",
    "\n",
    "generated_questions.reset_index(inplace=True, drop=True)\n",
    "\n",
    "display(\n",
    "    generated_questions[\n",
    "        [\n",
    "            \"question\",\n",
    "            \"answer\",\n",
    "            \"groundedness_score\",\n",
    "            \"relevance_score\",\n",
    "            \"standalone_score\",\n",
    "        ]\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "eval_dataset = datasets.Dataset.from_pandas(\n",
    "    generated_questions, split=\"train\", preserve_index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Push dataset to HuggingFace Hub for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "814ad46b2bad417a9fb11a2ec77f0a69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repository URL: https://huggingface.co/datasets/delayedkarma/litgpt_instruction_qa\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import create_repo\n",
    "from huggingface_hub import Repository\n",
    "\n",
    "repo_name = \"litgpt_instruction_qa\"  # Choose a name for your dataset repository\n",
    "repo_url = create_repo(repo_name, repo_type=\"dataset\")\n",
    "print(\"Repository URL:\", repo_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fa6c01d3f584cf4a33b268373e8e252",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4afea061777348198ca0bc86f57f2247",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/delayedkarma/litgpt_instruction_qa/commit/40ec15f700cffd95440825e20ac06ed05d6513a1', commit_message='Upload dataset', commit_description='', oid='40ec15f700cffd95440825e20ac06ed05d6513a1', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_dataset.push_to_hub(f\"delayedkarma/litgpt_instruction_qa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Dataset is now pushed to hub\n",
    "\n",
    "### You can load it using the following;::\n",
    "\n",
    "# eval_dataset = datasets.load_dataset(\"delayedkarma/litgpt_instruction_qa\", split=\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In Part 2: Build and evaluate a RAG system using the synthesized dataset (LLM-as-a-judge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cloudspace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
