{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective: To build a simple RAG evaluation framework\n",
    "\n",
    "### Part 2: Build & Benchmark a RAG System using an already synthesized evaluation dataset (in Part 1)\n",
    "\n",
    "#### Part 1: Synthesize and filter an Instruction dataset from a custom knowledge-base (See https://lightning.ai/panchamsnotes/studios/evaluate-your-rag-part-1-synthesize-an-evaluation-dataset?view=public&section=featured)\n",
    "\n",
    "\n",
    "\n",
    "#### Primary reference: https://huggingface.co/learn/cookbook/en/rag_evaluation by Aymeric Roucher (https://huggingface.co/m-ric)\n",
    "\n",
    "For the knowledge base, let us use the  litgpt Github repo: https://github.com/Lightning-AI/litgpt/tree/main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM Reader retrieves relevant documents to formulate response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installs and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q torch transformers transformers langchain sentence-transformers tqdm openpyxl openai pandas datasets\n",
    "%pip install -U --quiet langchain langsmith langchainhub langchain_benchmarks langchain-openai Gitpython python-dotenv RAGatouille\n",
    "%pip install --quiet chromadb openai huggingface pandas langchain_experimental sentence_transformers pyarrow anthropic tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "\n",
    "openai_api_key = os.environ['OPENAI_API_KEY'] \n",
    "hf_api_key = os.environ['HF_API_KEY'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import json\n",
    "import datasets\n",
    "import random\n",
    "import glob\n",
    "from typing import Optional, List, Tuple\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import GitLoader\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.docstore.document import Document as LangchainDocument\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores.utils import DistanceStrategy\n",
    "from langchain_community.llms import HuggingFaceHub\n",
    "from langchain_core.vectorstores import VectorStore\n",
    "from langchain_core.language_models.llms import LLM\n",
    "from langchain_core.language_models import BaseChatModel\n",
    "\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from langchain.schema import SystemMessage\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "from ragatouille import RAGPretrainedModel\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the RAG System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing documents to build the knowledge base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = GitLoader(\n",
    "    clone_url=\"https://github.com/Lightning-AI/litgpt\",\n",
    "    repo_path=\"./litgpt_data_github/\",\n",
    "    branch=\"main\",\n",
    "    file_filter=lambda file_path: file_path.endswith(\".md\") # Only get the markdown files\n",
    ")\n",
    "\n",
    "data = loader.load()\n",
    "\n",
    "RAW_KNOWLEDGE_BASE = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_documents_into_chunks(\n",
    "    chunk_size: int,\n",
    "    knowledge_base: List[LangchainDocument],\n",
    "    tokenizer_name: str,\n",
    ") -> List[LangchainDocument]:\n",
    "    \"\"\"\n",
    "    Split documents into chunks of size `chunk_size` characters and return a list of documents.\n",
    "    \"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(\n",
    "        AutoTokenizer.from_pretrained(tokenizer_name),\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=int(chunk_size / 10),\n",
    "        add_start_index=True,\n",
    "        strip_whitespace=True,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\", \"\\n\\n\\n\"],\n",
    "    )\n",
    "\n",
    "    docs_processed = []\n",
    "    for doc in knowledge_base:\n",
    "        docs_processed += text_splitter.split_documents([doc])\n",
    "\n",
    "    # Remove duplicates\n",
    "    unique_texts = {}\n",
    "    docs_processed_unique = []\n",
    "    for doc in docs_processed:\n",
    "        if doc.page_content not in unique_texts:\n",
    "            unique_texts[doc.page_content] = True\n",
    "            docs_processed_unique.append(doc)\n",
    "\n",
    "    return docs_processed_unique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the retriever after building a vector index using FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vector_index(\n",
    "    langchain_docs: List[LangchainDocument],\n",
    "    chunk_size: int,\n",
    "    embedding_model_name: Optional[str] = \"thenlper/gte-small\",\n",
    ") -> FAISS:\n",
    "    \"\"\"\n",
    "    Creates a FAISS index from the given embedding model and documents. Loads the index directly if it already exists.\n",
    "\n",
    "    Args:\n",
    "        langchain_docs: list of documents\n",
    "        chunk_size: size of the chunks to split the documents into\n",
    "        embedding_model_name: name of the embedding model to use\n",
    "\n",
    "    Returns:\n",
    "        FAISS index\n",
    "    \"\"\"\n",
    "    # load embedding_model\n",
    "    embedding_model = HuggingFaceEmbeddings(\n",
    "        model_name=embedding_model_name,\n",
    "        multi_process=True,\n",
    "        model_kwargs={\"device\": \"cuda\"},\n",
    "        encode_kwargs={\n",
    "            \"normalize_embeddings\": True\n",
    "        },  # set True to compute cosine similarity\n",
    "    )\n",
    "\n",
    "    # Check if embeddings already exist on disk\n",
    "    index_name = (\n",
    "        f\"index_chunk:{chunk_size}_embeddings:{embedding_model_name.replace('/', '~')}\"\n",
    "    )\n",
    "    index_folder_path = f\"./data/indexes/{index_name}/\"\n",
    "    if os.path.isdir(index_folder_path):\n",
    "        return FAISS.load_local(\n",
    "            index_folder_path,\n",
    "            embedding_model,\n",
    "            distance_strategy=DistanceStrategy.COSINE,\n",
    "            allow_dangerous_deserialization=True\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        print(\"Index not found, generating it...\")\n",
    "        docs_processed = split_documents_into_chunks(\n",
    "            chunk_size,\n",
    "            langchain_docs,\n",
    "            embedding_model_name,\n",
    "        )\n",
    "        knowledge_index = FAISS.from_documents(\n",
    "            docs_processed, embedding_model, distance_strategy=DistanceStrategy.COSINE\n",
    "        )\n",
    "        knowledge_index.save_local(index_folder_path)\n",
    "        return knowledge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAG_PROMPT_TEMPLATE = \"\"\"\n",
    "<|system|>\n",
    "Using the information contained in the context,\n",
    "give a comprehensive answer to the question.\n",
    "Respond only to the question asked, response should be concise and relevant to the question.\n",
    "Provide the number of the source document when relevant.\n",
    "If the answer cannot be deduced from the context, do not give an answer.</s>\n",
    "<|user|>\n",
    "Context:\n",
    "{context}\n",
    "---\n",
    "Now here is the question you need to answer.\n",
    "\n",
    "Question: {question}\n",
    "</s>\n",
    "<|assistant|>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "repo_id = \"HuggingFaceH4/zephyr-7b-beta\" \n",
    "READER_MODEL_NAME = \"zephyr-7b-beta\"\n",
    "\n",
    "READER_LLM = HuggingFaceHub(\n",
    "    repo_id=repo_id,\n",
    "    task=\"text-generation\",\n",
    "    model_kwargs={\n",
    "        \"max_new_tokens\": 512,\n",
    "        \"top_k\": 30,\n",
    "        \"temperature\": 0.1,\n",
    "        \"repetition_penalty\": 1.03,\n",
    "    },\n",
    "    huggingfacehub_api_token=hf_api_key\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rag_response(\n",
    "    question: str,\n",
    "    llm: LLM,\n",
    "    knowledge_index: VectorStore,\n",
    "    reranker: Optional[RAGPretrainedModel] = None,\n",
    "    num_retrieved_docs: int = 30,\n",
    "    num_docs_final: int = 7,\n",
    ") -> Tuple[str, List[LangchainDocument]]:\n",
    "    \"\"\"Answer a question using RAG with the given knowledge index.\"\"\"\n",
    "    # Gather documents with retriever\n",
    "    relevant_docs = knowledge_index.similarity_search(\n",
    "        query=question, k=num_retrieved_docs\n",
    "    )\n",
    "    relevant_docs = [doc.page_content for doc in relevant_docs]  # keep only the text\n",
    "\n",
    "    # Optionally rerank results\n",
    "    if reranker:\n",
    "        relevant_docs = reranker.rerank(question, relevant_docs, k=num_docs_final)\n",
    "        relevant_docs = [doc[\"content\"] for doc in relevant_docs]\n",
    "\n",
    "    relevant_docs = relevant_docs[:num_docs_final]\n",
    "\n",
    "    # Build the final prompt\n",
    "    context = \"\\nExtracted documents:\\n\"\n",
    "    context += \"\".join(\n",
    "        [f\"Document {str(i)}:::\\n\" + doc for i, doc in enumerate(relevant_docs)]\n",
    "    )\n",
    "\n",
    "    final_prompt = RAG_PROMPT_TEMPLATE.format(question=question, context=context)\n",
    "\n",
    "    # Redact an answer\n",
    "    answer = llm(final_prompt)\n",
    "\n",
    "    return answer, relevant_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark the RAG System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get eval dataset synthesized in Part 1\n",
    " (https://lightning.ai/panchamsnotes/studios/evaluate-your-rag-part-1-synthesize-an-evaluation-dataset?section=featured)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d294b59defbf47059ede834db0d32171",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### You can load it using the following;::\n",
    "\n",
    "eval_dataset = datasets.load_dataset(\"delayedkarma/litgpt_instruction_qa\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_rag_tests(\n",
    "    eval_dataset: datasets.Dataset,\n",
    "    llm: BaseChatModel,\n",
    "    knowledge_index: VectorStore,\n",
    "    output_file: str,\n",
    "    reranker: Optional[RAGPretrainedModel] = None,\n",
    "    verbose: Optional[bool] = True,\n",
    "    test_settings: Optional[str] = None,  # To document the test settings used\n",
    "):\n",
    "    \"\"\"Runs RAG tests on the given dataset and saves the results to the given output file.\"\"\"\n",
    "    try:  # load previous generations if they exist\n",
    "        with open(output_file, \"r\") as f:\n",
    "            outputs = json.load(f)\n",
    "    except:\n",
    "        outputs = []\n",
    "\n",
    "    for example in tqdm(eval_dataset):\n",
    "        question = example[\"question\"]\n",
    "        if question in [output[\"question\"] for output in outputs]:\n",
    "            continue\n",
    "\n",
    "        answer, relevant_docs = get_rag_response(\n",
    "            question, llm, knowledge_index, reranker=reranker\n",
    "        )\n",
    "        if verbose:\n",
    "            print(\"=======================================================\")\n",
    "            print(f\"Question: {question}\")\n",
    "            print(f\"Answer: {answer}\")\n",
    "            print(f'True answer: {example[\"answer\"]}')\n",
    "        result = {\n",
    "            \"question\": question,\n",
    "            \"true_answer\": example[\"answer\"],\n",
    "            \"source_doc\": example[\"source_doc\"],\n",
    "            \"generated_answer\": answer,\n",
    "            \"retrieved_docs\": [doc for doc in relevant_docs],\n",
    "        }\n",
    "        if test_settings:\n",
    "            result[\"test_settings\"] = test_settings\n",
    "        outputs.append(result)\n",
    "\n",
    "        with open(output_file, \"w\") as f:\n",
    "            json.dump(outputs, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define evaluation prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "EVALUATION_PROMPT = \"\"\"###Task Description:\n",
    "An instruction (might include an Input inside it), a response to evaluate, a reference answer that gets a score of 5, and a score rubric representing a evaluation criteria are given.\n",
    "1. Write a detailed feedback that assess the quality of the response strictly based on the given score rubric, not evaluating in general.\n",
    "2. After writing a feedback, write a score that is an integer between 1 and 5. You should refer to the score rubric.\n",
    "3. The output format should look as follows: \\\"Feedback: {{write a feedback for criteria}} [RESULT] {{an integer number between 1 and 5}}\\\"\n",
    "4. Please do not generate any other opening, closing, and explanations. Be sure to include [RESULT] in your output.\n",
    "\n",
    "###The instruction to evaluate:\n",
    "{instruction}\n",
    "\n",
    "###Response to evaluate:\n",
    "{response}\n",
    "\n",
    "###Reference Answer (Score 5):\n",
    "{reference_answer}\n",
    "\n",
    "###Score Rubrics:\n",
    "[Is the response correct, accurate, and factual based on the reference answer?]\n",
    "Score 1: The response is completely incorrect, inaccurate, and/or not factual.\n",
    "Score 2: The response is mostly incorrect, inaccurate, and/or not factual.\n",
    "Score 3: The response is somewhat correct, accurate, and/or factual.\n",
    "Score 4: The response is mostly correct, accurate, and factual.\n",
    "Score 5: The response is completely correct, accurate, and factual.\n",
    "\n",
    "###Feedback:\"\"\"\n",
    "\n",
    "\n",
    "evaluation_prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(content=\"You are a fair evaluator language model.\"),\n",
    "        HumanMessagePromptTemplate.from_template(EVALUATION_PROMPT),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define evaluator models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_chat_model_gpt4_1106 = ChatOpenAI(model=\"gpt-4-1106-preview\", temperature=0)\n",
    "evaluator_name_gpt4_1106 = \"GPT4_1106\"\n",
    "\n",
    "eval_chat_model_gpt4_0125 = ChatOpenAI(model=\"gpt-4-0125-preview\", temperature=0)\n",
    "evaluator_name_gpt4_0125 = \"GPT4_0125\"\n",
    "\n",
    "\n",
    "def evaluate_rag_responses(\n",
    "    answer_path: str,\n",
    "    eval_chat_model: BaseChatModel,\n",
    "    evaluator_name: str,\n",
    "    evaluation_prompt_template: ChatPromptTemplate,\n",
    ") -> None:\n",
    "    \"\"\"Evaluates generated answers. Modifies the given answer file in place for better checkpointing.\"\"\"\n",
    "    answers = []\n",
    "    if os.path.isfile(answer_path):  # load previous generations if they exist\n",
    "        answers = json.load(open(answer_path, \"r\"))\n",
    "\n",
    "    for experiment in tqdm(answers):\n",
    "        if f\"eval_score_{evaluator_name}\" in experiment:\n",
    "            continue\n",
    "\n",
    "        eval_prompt = evaluation_prompt_template.format_messages(\n",
    "            instruction=experiment[\"question\"],\n",
    "            response=experiment[\"generated_answer\"],\n",
    "            reference_answer=experiment[\"true_answer\"],\n",
    "        )\n",
    "        eval_result = eval_chat_model.invoke(eval_prompt)\n",
    "        feedback, score = [\n",
    "            item.strip() for item in eval_result.content.split(\"[RESULT]\")\n",
    "        ]\n",
    "        experiment[f\"eval_score_{evaluator_name}\"] = score\n",
    "        experiment[f\"eval_feedback_{evaluator_name}\"] = feedback\n",
    "\n",
    "        with open(answer_path, \"w\") as f:\n",
    "            json.dump(answers, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the tests and evaluate the responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation for chunk:200_embeddings:thenlper~gte-small_rerank:True_reader-model:zephyr-7b-beta:\n",
      "Loading knowledge base embeddings...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running RAG...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 11912.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation for gpt-4-0125-preview ...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [02:06<00:00,  4.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation for gpt-4-1106-preview...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [02:15<00:00,  4.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation for chunk:200_embeddings:thenlper~gte-small_rerank:False_reader-model:zephyr-7b-beta:\n",
      "Loading knowledge base embeddings...\n",
      "Running RAG...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 11516.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation for gpt-4-0125-preview ...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [02:06<00:00,  4.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation for gpt-4-1106-preview...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [02:12<00:00,  4.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation for chunk:200_embeddings:BAAI~bge-small-en-v1.5_rerank:True_reader-model:zephyr-7b-beta:\n",
      "Loading knowledge base embeddings...\n",
      "Running RAG...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/30 [00:00<?, ?it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 22.87it/s]\n",
      " 17%|█▋        | 5/30 [00:06<00:31,  1.25s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00, 22.28it/s]\n",
      " 20%|██        | 6/30 [00:13<01:02,  2.60s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00, 22.79it/s]\n",
      " 23%|██▎       | 7/30 [00:18<01:11,  3.11s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00, 22.60it/s]\n",
      " 27%|██▋       | 8/30 [00:29<01:53,  5.16s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00, 22.75it/s]\n",
      " 30%|███       | 9/30 [00:33<01:44,  4.97s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00, 22.27it/s]\n",
      " 33%|███▎      | 10/30 [00:44<02:09,  6.49s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00, 22.30it/s]\n",
      " 37%|███▋      | 11/30 [00:52<02:10,  6.87s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00, 22.40it/s]\n",
      " 40%|████      | 12/30 [01:08<02:52,  9.61s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00, 22.65it/s]\n",
      " 43%|████▎     | 13/30 [01:16<02:35,  9.15s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00, 23.08it/s]\n",
      " 47%|████▋     | 14/30 [01:26<02:29,  9.36s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00, 22.83it/s]\n",
      " 50%|█████     | 15/30 [01:32<02:07,  8.51s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00, 23.18it/s]\n",
      " 53%|█████▎    | 16/30 [01:45<02:16,  9.74s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00, 22.81it/s]\n",
      " 57%|█████▋    | 17/30 [01:49<01:45,  8.09s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00, 23.09it/s]\n",
      " 60%|██████    | 18/30 [01:57<01:35,  7.99s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00, 21.46it/s]\n",
      " 63%|██████▎   | 19/30 [02:03<01:20,  7.28s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00, 22.81it/s]\n",
      " 67%|██████▋   | 20/30 [02:12<01:19,  7.98s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00, 23.12it/s]\n",
      " 70%|███████   | 21/30 [02:31<01:39, 11.09s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00, 23.73it/s]\n",
      " 73%|███████▎  | 22/30 [02:43<01:32, 11.53s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00, 21.98it/s]\n",
      " 77%|███████▋  | 23/30 [02:54<01:19, 11.30s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00, 23.23it/s]\n",
      " 80%|████████  | 24/30 [03:16<01:26, 14.43s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00, 24.08it/s]\n",
      " 83%|████████▎ | 25/30 [03:25<01:05, 13.03s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00, 22.13it/s]\n",
      " 87%|████████▋ | 26/30 [03:34<00:47, 11.77s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00, 22.92it/s]\n",
      " 90%|█████████ | 27/30 [03:39<00:29,  9.79s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00, 23.47it/s]\n",
      " 93%|█████████▎| 28/30 [03:44<00:16,  8.27s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00, 22.26it/s]\n",
      " 97%|█████████▋| 29/30 [03:53<00:08,  8.53s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00, 22.25it/s]\n",
      "100%|██████████| 30/30 [04:01<00:00,  8.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation for gpt-4-0125-preview ...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [02:18<00:00,  4.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation for gpt-4-1106-preview...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [02:08<00:00,  4.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation for chunk:200_embeddings:BAAI~bge-small-en-v1.5_rerank:False_reader-model:zephyr-7b-beta:\n",
      "Loading knowledge base embeddings...\n",
      "Running RAG...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [04:25<00:00,  8.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation for gpt-4-0125-preview ...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [02:09<00:00,  4.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation for gpt-4-1106-preview...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [01:58<00:00,  3.95s/it]\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(\"./output\"):\n",
    "    os.mkdir(\"./output\")\n",
    "\n",
    "for chunk_size in [200]:  # Add other chunk sizes (in tokens) as needed\n",
    "    for embeddings in [\"thenlper/gte-small\", \"BAAI/bge-small-en-v1.5\"]:  # Add other embeddings as needed\n",
    "        for rerank in [True, False]:\n",
    "            settings_name = f\"chunk:{chunk_size}_embeddings:{embeddings.replace('/', '~')}_rerank:{rerank}_reader-model:{READER_MODEL_NAME}\"\n",
    "            output_file_name = f\"./output/rag_{settings_name}.json\"\n",
    "\n",
    "            print(f\"Running evaluation for {settings_name}:\")\n",
    "\n",
    "            print(\"Loading knowledge base embeddings...\")\n",
    "            knowledge_index = create_vector_index(\n",
    "                RAW_KNOWLEDGE_BASE,\n",
    "                chunk_size=chunk_size,\n",
    "                embedding_model_name=embeddings\n",
    "            )\n",
    "\n",
    "            print(\"Running RAG...\")\n",
    "            reranker = (\n",
    "                RAGPretrainedModel.from_pretrained(\"colbert-ir/colbertv2.0\")\n",
    "                if rerank\n",
    "                else None\n",
    "            )\n",
    "            run_rag_tests(\n",
    "                eval_dataset=eval_dataset,\n",
    "                llm=READER_LLM,\n",
    "                knowledge_index=knowledge_index,\n",
    "                output_file=output_file_name,\n",
    "                reranker=reranker,\n",
    "                verbose=False,\n",
    "                test_settings=settings_name,\n",
    "            )\n",
    "\n",
    "            print(\"Running evaluation for gpt-4-0125-preview ...\")\n",
    "            print()\n",
    "            evaluate_rag_responses(\n",
    "                output_file_name,\n",
    "                eval_chat_model_gpt4_0125,\n",
    "                evaluator_name_gpt4_0125,\n",
    "                evaluation_prompt_template,\n",
    "            )\n",
    "\n",
    "            print(\"Running evaluation for gpt-4-1106-preview ...\")\n",
    "            print()\n",
    "            evaluate_rag_responses(\n",
    "                output_file_name,\n",
    "                eval_chat_model_gpt4_1106,\n",
    "                evaluator_name_gpt4_1106,\n",
    "                evaluation_prompt_template,\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./output/rag_chunk:200_embeddings:thenlper~gte-small_rerank:False_reader-model:zephyr-7b-beta.json\n",
      "./output/rag_chunk:200_embeddings:thenlper~gte-small_rerank:True_reader-model:zephyr-7b-beta.json\n",
      "./output/rag_chunk:200_embeddings:BAAI~bge-small-en-v1.5_rerank:True_reader-model:zephyr-7b-beta.json\n",
      "./output/rag_chunk:200_embeddings:BAAI~bge-small-en-v1.5_rerank:False_reader-model:zephyr-7b-beta.json\n"
     ]
    }
   ],
   "source": [
    "outputs = []\n",
    "\n",
    "for file in glob.glob(\"./output/*.json\"):\n",
    "    print(file)\n",
    "    output = pd.DataFrame(json.load(open(file, \"r\")))\n",
    "    output[\"settings\"] = file\n",
    "    outputs.append(output)\n",
    "result = pd.concat(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['question', 'true_answer', 'source_doc', 'generated_answer',\n",
       "       'retrieved_docs', 'test_settings', 'eval_score_GPT4_0125',\n",
       "       'eval_feedback_GPT4_0125', 'eval_score_GPT4_1106',\n",
       "       'eval_feedback_GPT4_1106', 'settings'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# result.drop(['eval_score_GPT35', 'eval_feedback_GPT35','eval_score_GPT4', 'eval_feedback_GPT4'], axis=1, inplace=True) # artifacts from previous run\n",
    "result.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>true_answer</th>\n",
       "      <th>source_doc</th>\n",
       "      <th>generated_answer</th>\n",
       "      <th>retrieved_docs</th>\n",
       "      <th>test_settings</th>\n",
       "      <th>eval_score_GPT4_0125</th>\n",
       "      <th>eval_feedback_GPT4_0125</th>\n",
       "      <th>eval_score_GPT4_1106</th>\n",
       "      <th>eval_feedback_GPT4_1106</th>\n",
       "      <th>settings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the memory usage of Llama 2 with 7B when using bnb.nf4-dq?\\n</td>\n",
       "      <td>13.84 GB</td>\n",
       "      <td>tutorials/resource-tables.md</td>\n",
       "      <td>\\n&lt;|system|&gt;\\nUsing the information contained in the context,\\ngive a comprehensive answer to the question.\\nRespond only to the question asked, response should be concise and relevant to the question.\\nProvide the number of the source document when relevant.\\nIf the answer cannot be deduced from the context, do not give an answer.&lt;/s&gt;\\n&lt;|user|&gt;\\nContext:\\n\\nExtracted documents:\\nDocument 0:::\\n|       |                |              |                 |                      |             |                    |\\n| 7 B   | Llama 2        | None         | 1               | 4,194,304            | 21.30 GB    | 2.36 min           |\\n| 7 B   | Llama 2        | bnb.nf4      | 1               | 4,194,304            | 14.14 GB    | 3.68 min           |\\n| 7 B   | Llama 2        | bnb.nf4-dq   | 1               | 4,194,304            | 13.84 GB    | 3.83 min           |\\n| 7 B   | Llama 2        | None         | 2               | 4,194,304            | 29.07 GB    | 2.52 min           |\\n| 7 B   | Llama 2        | None         | 4               | 4,194,304            | OOM         | -                  |\\n|       |                |              |                 |                      |             |                    |Document 1:::\\n|       |                |              |          |                                           |           |\\n| 7 B   | Llama 2        | None         | 1 x A100 | 13.52 GB                                  | 30.97     |\\n| 7 B   | Llama 2        | bnb.nf4      | 1 x A100 | 4.57 GB                                   | 19.98     |\\n| 7 B   | Llama 2        | bnb.nf4-dq   | 1 x A100 | 4.26 GB                                   | 17.3      |\\n|       |                |              |          |                                           |           |\\n| 13 B  | Llama 2        | None         | 1 x A100 | 26.21 GB                                  | 24.82     |\\n| 13 B  | Llama 2        | bnb.nf4      | 1 x A100 | 8.32 GB                                   | 16.73     |Document 2:::\\n|       |                |              |                 |                      |             |                    |\\n| 13 B  | Llama 2        | None         | 1               | 6,553,600            | 38.12 GB    | 3.19 min           |\\n| 13 B  | Llama 2        | bnb.nf4      | 1               | 6,553,600            | 23.14 GB    | 6.38 min           |\\n| 13 B  | Llama 2        | bnb.nf4-dq   | 1               | 6,553,600            | 22.55 GB    | 6.55 min           |\\n| 13 B  | Llama 2        | None         | 2               | 6,553,600            | OOM         | -                  |\\n| 13 B  | Llama 2        | None         | 4               | 6,553,600            | OOM         | -                  |\\n|       |                |              |                 |                      |             |                    |Document 3:::\\n| 13 B  | Llama 2        | bnb.nf4-dq   | 1 x A100 | 7.72 GB                                   | 14.43     |\\n|       |                |              |          |                                           |           |\\n| 34 B  | CodeLlama      | None         | 1 x A100 | OOM                                       | -         |\\n| 34 B  | CodeLlama      | bnb.nf4      | 1 x A100 | 20.52 GB                                  | 14.32     |\\n| 34 B  | CodeLlama      | bnb.nf4-dq   | 1 x A100 | 18.95 GB                                  | 12.37     |\\n|       |                |              |          |                                           |           |\\n| 40 B  | Falcon         | None         | 1 x A100 | OOM                                       | -         |\\n| 40 B  | Falcon         | bnb.nf4      | 1 x A100 | 26.55 GB                                  | 13.25     |Document 4:::\\n| 13 B  | Llama 2        | bnb.nf4      | 1               | 6,553,600            | 2 x A100 | N/A         | -                  |\\n| 13 B  | Llama 2        | bnb.nf4-dq   | 1               | 6,553,600            | 2 x A100 | N/A         | -                  |\\n|       |                |              |                 |                      |          |             |                    |\\n| 13 B  | Llama 2        | None         | 1               | 6,553,600            | 4 x A100 | 35.57 GB    | 10.25 min          |\\n| 40 B  | Falcon         | None         | 1               | 12,042,240           | 4 x A100 | OOM         | -                  |Document 5:::\\n| 3 B  | StableLM Alpha | bnb.nf4      | 1               | 2,125,248            | 7.41 GB     | 1.59 min           |\\n| 3 B  | StableLM Alpha | bnb.nf4-dq   | 1               | 2,125,248            | 7.25 GB     | 1.62 min           |\\n|      |                |              |                 |                      |             |                    |\\n| 7 B  | Llama 2        | None         | 1               | 4,279,744            | 25.51 GB    | 1.81 min           |\\n| 7 B  | Llama 2        | bnb.nf4      | 1               | 4,279,744            | 18.30 GB    | 3.23 min           |\\n| 7 B  | Llama 2        | bnb.nf4-dq   | 1               | 4,279,744            | 17.98 GB    | 3.32 min           |Document 6:::\\n| 3 B  | StableLM Alpha | bnb.nf4      | 1               | 573,888              | 5.65 GB     | 1.38 min           |\\n| 3 B  | StableLM Alpha | bnb.nf4-dq   | 1               | 573,888              | 5.48 GB     | 1.46 min           |\\n|      |                |              |                 |                      |             |                    |\\n| 7 B  | Llama 2        | None         | 1               | 1,229,760            | 19.98 GB    | 1.50 min           |\\n| 7 B  | Llama 2        | bnb.nf4      | 1               | 1,229,760            | 12.68 GB    | 2.93 min           |\\n| 7 B  | Llama 2        | bnb.nf4-dq   | 1               | 1,229,760            | 12.38 GB    | 3.00 min           |\\n---\\nNow here is the question you need to answer.\\n\\nQuestion: What is the memory usage of Llama 2 with 7B when using bnb.nf4-dq?\\n\\n&lt;/s&gt;\\n&lt;|assistant|&gt;\\nThe memory usage of Llama 2 with 7B when using bnb.nf4-dq is 4,194,304 bytes, as shown in Document 0, row 3. However, in Document 1, row 3, it is also listed as 4,26 GB, which is equivalent to 36,864,000,000 bytes. This discrepancy may be due to differences in the way memory is reported or measured in these documents. In either case, the memory usage is significantly lower than when using bnb.nf4 (14.14 GB) or None (29.07 GB) in Document 0.</td>\n",
       "      <td>[|       |                |              |                 |                      |             |                    |\\n| 7 B   | Llama 2        | None         | 1               | 4,194,304            | 21.30 GB    | 2.36 min           |\\n| 7 B   | Llama 2        | bnb.nf4      | 1               | 4,194,304            | 14.14 GB    | 3.68 min           |\\n| 7 B   | Llama 2        | bnb.nf4-dq   | 1               | 4,194,304            | 13.84 GB    | 3.83 min           |\\n| 7 B   | Llama 2        | None         | 2               | 4,194,304            | 29.07 GB    | 2.52 min           |\\n| 7 B   | Llama 2        | None         | 4               | 4,194,304            | OOM         | -                  |\\n|       |                |              |                 |                      |             |                    |, |       |                |              |          |                                           |           |\\n| 7 B   | Llama 2        | None         | 1 x A100 | 13.52 GB                                  | 30.97     |\\n| 7 B   | Llama 2        | bnb.nf4      | 1 x A100 | 4.57 GB                                   | 19.98     |\\n| 7 B   | Llama 2        | bnb.nf4-dq   | 1 x A100 | 4.26 GB                                   | 17.3      |\\n|       |                |              |          |                                           |           |\\n| 13 B  | Llama 2        | None         | 1 x A100 | 26.21 GB                                  | 24.82     |\\n| 13 B  | Llama 2        | bnb.nf4      | 1 x A100 | 8.32 GB                                   | 16.73     |, |       |                |              |                 |                      |             |                    |\\n| 13 B  | Llama 2        | None         | 1               | 6,553,600            | 38.12 GB    | 3.19 min           |\\n| 13 B  | Llama 2        | bnb.nf4      | 1               | 6,553,600            | 23.14 GB    | 6.38 min           |\\n| 13 B  | Llama 2        | bnb.nf4-dq   | 1               | 6,553,600            | 22.55 GB    | 6.55 min           |\\n| 13 B  | Llama 2        | None         | 2               | 6,553,600            | OOM         | -                  |\\n| 13 B  | Llama 2        | None         | 4               | 6,553,600            | OOM         | -                  |\\n|       |                |              |                 |                      |             |                    |, | 13 B  | Llama 2        | bnb.nf4-dq   | 1 x A100 | 7.72 GB                                   | 14.43     |\\n|       |                |              |          |                                           |           |\\n| 34 B  | CodeLlama      | None         | 1 x A100 | OOM                                       | -         |\\n| 34 B  | CodeLlama      | bnb.nf4      | 1 x A100 | 20.52 GB                                  | 14.32     |\\n| 34 B  | CodeLlama      | bnb.nf4-dq   | 1 x A100 | 18.95 GB                                  | 12.37     |\\n|       |                |              |          |                                           |           |\\n| 40 B  | Falcon         | None         | 1 x A100 | OOM                                       | -         |\\n| 40 B  | Falcon         | bnb.nf4      | 1 x A100 | 26.55 GB                                  | 13.25     |, | 13 B  | Llama 2        | bnb.nf4      | 1               | 6,553,600            | 2 x A100 | N/A         | -                  |\\n| 13 B  | Llama 2        | bnb.nf4-dq   | 1               | 6,553,600            | 2 x A100 | N/A         | -                  |\\n|       |                |              |                 |                      |          |             |                    |\\n| 13 B  | Llama 2        | None         | 1               | 6,553,600            | 4 x A100 | 35.57 GB    | 10.25 min          |\\n| 40 B  | Falcon         | None         | 1               | 12,042,240           | 4 x A100 | OOM         | -                  |, | 3 B  | StableLM Alpha | bnb.nf4      | 1               | 2,125,248            | 7.41 GB     | 1.59 min           |\\n| 3 B  | StableLM Alpha | bnb.nf4-dq   | 1               | 2,125,248            | 7.25 GB     | 1.62 min           |\\n|      |                |              |                 |                      |             |                    |\\n| 7 B  | Llama 2        | None         | 1               | 4,279,744            | 25.51 GB    | 1.81 min           |\\n| 7 B  | Llama 2        | bnb.nf4      | 1               | 4,279,744            | 18.30 GB    | 3.23 min           |\\n| 7 B  | Llama 2        | bnb.nf4-dq   | 1               | 4,279,744            | 17.98 GB    | 3.32 min           |, | 3 B  | StableLM Alpha | bnb.nf4      | 1               | 573,888              | 5.65 GB     | 1.38 min           |\\n| 3 B  | StableLM Alpha | bnb.nf4-dq   | 1               | 573,888              | 5.48 GB     | 1.46 min           |\\n|      |                |              |                 |                      |             |                    |\\n| 7 B  | Llama 2        | None         | 1               | 1,229,760            | 19.98 GB    | 1.50 min           |\\n| 7 B  | Llama 2        | bnb.nf4      | 1               | 1,229,760            | 12.68 GB    | 2.93 min           |\\n| 7 B  | Llama 2        | bnb.nf4-dq   | 1               | 1,229,760            | 12.38 GB    | 3.00 min           |]</td>\n",
       "      <td>chunk:200_embeddings:thenlper~gte-small_rerank:False_reader-model:zephyr-7b-beta</td>\n",
       "      <td>1</td>\n",
       "      <td>Feedback: The response provided incorrect information by stating the memory usage in bytes and then incorrectly converting those bytes into gigabytes, leading to a significant discrepancy. The correct answer, as per the reference answer, is 13.84 GB, which is directly mentioned in the provided context (Document 0, row 3). The response failed to accurately report this figure and instead introduced confusion with incorrect data conversion. This deviation from the factual content in the context and the reference answer indicates a lack of accuracy in the response.</td>\n",
       "      <td>1</td>\n",
       "      <td>The response provided is incorrect and not factual as per the reference answer. The response cites two different memory usages, 4,194,304 bytes and 4.26 GB, which are both incorrect and do not match the reference answer of 13.84 GB. The correct memory usage is clearly stated in the context provided in Document 0, row 3, which should have been the sole focus of the answer. The response also introduces unnecessary confusion by mentioning a discrepancy without clarifying that the correct answer is 13.84 GB as per the reference answer. Therefore, the response does not meet the criteria for accuracy and factuality.</td>\n",
       "      <td>./output/rag_chunk:200_embeddings:thenlper~gte-small_rerank:False_reader-model:zephyr-7b-beta.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the command to run the evaluation harness?\\n</td>\n",
       "      <td>The command to run the evaluation harness is `lm_eval --model hf --model_args pretrained=out/hf-tinyllama/converted --tasks \"hellaswag,gsm8k,truthfulqa_mc2,mmlu,winogrande,arc_challenge\" --device \"cuda:0\" --batch_size 4`.</td>\n",
       "      <td>tutorials/convert_lit_models.md</td>\n",
       "      <td>\\n&lt;|system|&gt;\\nUsing the information contained in the context,\\ngive a comprehensive answer to the question.\\nRespond only to the question asked, response should be concise and relevant to the question.\\nProvide the number of the source document when relevant.\\nIf the answer cannot be deduced from the context, do not give an answer.&lt;/s&gt;\\n&lt;|user|&gt;\\nContext:\\n\\nExtracted documents:\\nDocument 0:::\\nThen, we can run the Evaluation Harness as follows:\\n\\n```bash\\nlm_eval --model hf \\\\n    --model_args pretrained=\"out/converted_model\" \\\\n    --tasks \"hellaswag,gsm8k,truthfulqa_mc2,mmlu,winogrande,arc_challenge\" \\\\n    --device \"cuda:0\" \\\\n    --batch_size 4\\n```\\n\\n&amp;nbsp;\\n\\n&gt; [!TIP]\\n&gt; The Evaluation Harness tasks above are those used in Open LLM Leaderboard. You can find a list all supported tasks [here](https://github.com/EleutherAI/lm-evaluation-harness/blob/master/docs/task_table.md).Document 1:::\\n4. Run the evaluation harness, for example:\\n\\n```bash\\nlm_eval --model hf \\\\n    --model_args pretrained=out/hf-tinyllama/converted \\\\n    --tasks \"hellaswag,gsm8k,truthfulqa_mc2,mmlu,winogrande,arc_challenge\" \\\\n    --device \"cuda:0\" \\\\n    --batch_size 4\\n```Document 2:::\\n# LLM Evaluation\\n\\n&amp;nbsp;\\n\\n## Using lm-evaluation-harness\\n\\nYou can evaluate LitGPT using [EleutherAI's lm-eval](https://github.com/EleutherAI/lm-evaluation-harness) framework with a large number of different evaluation tasks.\\n\\nYou need to install the `lm-eval` framework first:\\n\\n```bash\\npip install lm_eval\\n```\\n\\n&amp;nbsp;\\n\\n### Evaluating LitGPT base models\\n\\nSuppose you downloaded a base model that we want to evaluate. Here, we use the `microsoft/phi-2` model:\\n\\n```bash\\nlitgpt download --repo_id microsoft/phi-2\\n```Document 3:::\\nYou can then use the model with external tools, for example, Eleuther AI's [LM Evaluation Harness](https://github.com/EleutherAI/lm-evaluation-harness) (see the `lm_eval` installation instructions [here](https://github.com/EleutherAI/lm-evaluation-harness?tab=readme-ov-file#install)).\\n\\nThe LM Evaluation Harness requires a tokenizer to be present in the model checkpoint folder, which we can copy from the original download checkpoint:\\n\\n```bash\\n# Copy the tokenizer needed by the Eval Harness\\ncp checkpoints/microsoft/phi-2/tokenizer*\\nout/converted_model\\n```\\n\\nThen, we can run the Evaluation Harness as follows:Document 4:::\\n&amp;nbsp;\\n## Evaluating models\\n\\nLitGPT comes with a handy `litgpt evaluate` command to evaluate models with [Eleuther AI's Evaluation Harness](https://github.com/EleutherAI/lm-evaluation-harness). For example, to evaluate the previously downloaded `microsoft/phi-2` model on several tasks available from the Evaluation Harness, you can use the following command:\\n\\n```bash\\nlitgpt evaluate \\\\n  --checkpoint_dir checkpoints/microsoft/phi-2\\n  --batch_size 16 \\\\n  --tasks \"hellaswag,gsm8k,truthfulqa_mc2,mmlu,winogrande,arc_challenge\"\\n```Document 5:::\\nPlease note that the `litgpt evaluate` command run an internal model conversion. \\nThis is only necessary the first time you want to evaluate a model, and it will skip the\\nconversion steps if you run the `litgpt evaluate` on the same checkpint directory again.\\n\\nIn some cases, for example, if you modified the model in the `checkpoint_dir` since the first `litgpt evaluate`\\ncall, you need to use the `--force_conversion` flag to to update the files used by litgpt evaluate accordingly: \\n\\n```\\nlitgpt evaluate \\\\n  --checkpoint_dir checkpoints/microsoft/phi-2/ \\\\n  --batch_size 4 \\\\n  --out_dir evaluate_model/ \\\\n  --tasks \"hellaswag,truthfulqa_mc2,mmlu\" \\\\n  --force_conversion true\\n```Document 6:::\\n(A list of supported tasks can be found [here](https://github.com/EleutherAI/lm-evaluation-harness/blob/master/docs/task_table.md).)\\n\\n\\n&amp;nbsp;\\n## Deploy LLMs\\n\\nYou can deploy LitGPT LLMs using your tool of choice. Below is an example using LitGPT built-in serving capabilities:\\n\\n\\n```bash\\n# 1) Download a pretrained model (alternatively, use your own finetuned model)\\nlitgpt download --repo_id microsoft/phi-2\\n\\n# 2) Start the server\\nlitgpt serve --checkpoint_dir checkpoints/microsoft/phi-2\\n```\\n---\\nNow here is the question you need to answer.\\n\\nQuestion: What is the command to run the evaluation harness?\\n\\n&lt;/s&gt;\\n&lt;|assistant|&gt;\\nThe command to run the evaluation harness using LitGPT is:\\n\\n```bash\\nlitgpt evaluate \\\\n  --checkpoint_dir checkpoints/[MODEL_NAME] \\\\n  --batch_size [BATCH_SIZE] \\\\n  --tasks \"[TASK_1],[TASK_2],[TASK_3]\" \\\\n  [OPTIONAL_FLAGS]\\n```\\n\\nReplace `[MODEL_NAME]` with the name of the pretrained model you want to evaluate, `[BATCH_SIZE]` with the desired batch size, `[TASK_1],[TASK_2],[TASK_3]` with the names of the evaluation tasks you want to run (separated by commas), and `[OPTIONAL_FLAGS]` with any additional flags you want to pass to the `litgpt evaluate` command, such as `--out_dir` to specify an output directory or `--force_conversion` to force model conversion.\\n\\nNote that the `litgpt evaluate` command automatically converts the model for use with the Evaluation Harness if it hasn't been done before. If you modify the model in the `checkpoint_dir` since the last `litgpt evaluate` call, you can use the `--force_conversion` flag to update the files used by `litgpt evaluate`.\\n\\nA list of supported evaluation tasks can be found at https://github.com/EleutherAI/lm-evaluation-harness/blob/master/docs/task_table.md.</td>\n",
       "      <td>[Then, we can run the Evaluation Harness as follows:\\n\\n```bash\\nlm_eval --model hf \\\\n    --model_args pretrained=\"out/converted_model\" \\\\n    --tasks \"hellaswag,gsm8k,truthfulqa_mc2,mmlu,winogrande,arc_challenge\" \\\\n    --device \"cuda:0\" \\\\n    --batch_size 4\\n```\\n\\n&amp;nbsp;\\n\\n&gt; [!TIP]\\n&gt; The Evaluation Harness tasks above are those used in Open LLM Leaderboard. You can find a list all supported tasks [here](https://github.com/EleutherAI/lm-evaluation-harness/blob/master/docs/task_table.md)., 4. Run the evaluation harness, for example:\\n\\n```bash\\nlm_eval --model hf \\\\n    --model_args pretrained=out/hf-tinyllama/converted \\\\n    --tasks \"hellaswag,gsm8k,truthfulqa_mc2,mmlu,winogrande,arc_challenge\" \\\\n    --device \"cuda:0\" \\\\n    --batch_size 4\\n```, # LLM Evaluation\\n\\n&amp;nbsp;\\n\\n## Using lm-evaluation-harness\\n\\nYou can evaluate LitGPT using [EleutherAI's lm-eval](https://github.com/EleutherAI/lm-evaluation-harness) framework with a large number of different evaluation tasks.\\n\\nYou need to install the `lm-eval` framework first:\\n\\n```bash\\npip install lm_eval\\n```\\n\\n&amp;nbsp;\\n\\n### Evaluating LitGPT base models\\n\\nSuppose you downloaded a base model that we want to evaluate. Here, we use the `microsoft/phi-2` model:\\n\\n```bash\\nlitgpt download --repo_id microsoft/phi-2\\n```, You can then use the model with external tools, for example, Eleuther AI's [LM Evaluation Harness](https://github.com/EleutherAI/lm-evaluation-harness) (see the `lm_eval` installation instructions [here](https://github.com/EleutherAI/lm-evaluation-harness?tab=readme-ov-file#install)).\\n\\nThe LM Evaluation Harness requires a tokenizer to be present in the model checkpoint folder, which we can copy from the original download checkpoint:\\n\\n```bash\\n# Copy the tokenizer needed by the Eval Harness\\ncp checkpoints/microsoft/phi-2/tokenizer*\\nout/converted_model\\n```\\n\\nThen, we can run the Evaluation Harness as follows:, &amp;nbsp;\\n## Evaluating models\\n\\nLitGPT comes with a handy `litgpt evaluate` command to evaluate models with [Eleuther AI's Evaluation Harness](https://github.com/EleutherAI/lm-evaluation-harness). For example, to evaluate the previously downloaded `microsoft/phi-2` model on several tasks available from the Evaluation Harness, you can use the following command:\\n\\n```bash\\nlitgpt evaluate \\\\n  --checkpoint_dir checkpoints/microsoft/phi-2\\n  --batch_size 16 \\\\n  --tasks \"hellaswag,gsm8k,truthfulqa_mc2,mmlu,winogrande,arc_challenge\"\\n```, Please note that the `litgpt evaluate` command run an internal model conversion. \\nThis is only necessary the first time you want to evaluate a model, and it will skip the\\nconversion steps if you run the `litgpt evaluate` on the same checkpint directory again.\\n\\nIn some cases, for example, if you modified the model in the `checkpoint_dir` since the first `litgpt evaluate`\\ncall, you need to use the `--force_conversion` flag to to update the files used by litgpt evaluate accordingly: \\n\\n```\\nlitgpt evaluate \\\\n  --checkpoint_dir checkpoints/microsoft/phi-2/ \\\\n  --batch_size 4 \\\\n  --out_dir evaluate_model/ \\\\n  --tasks \"hellaswag,truthfulqa_mc2,mmlu\" \\\\n  --force_conversion true\\n```, (A list of supported tasks can be found [here](https://github.com/EleutherAI/lm-evaluation-harness/blob/master/docs/task_table.md).)\\n\\n\\n&amp;nbsp;\\n## Deploy LLMs\\n\\nYou can deploy LitGPT LLMs using your tool of choice. Below is an example using LitGPT built-in serving capabilities:\\n\\n\\n```bash\\n# 1) Download a pretrained model (alternatively, use your own finetuned model)\\nlitgpt download --repo_id microsoft/phi-2\\n\\n# 2) Start the server\\nlitgpt serve --checkpoint_dir checkpoints/microsoft/phi-2\\n```]</td>\n",
       "      <td>chunk:200_embeddings:thenlper~gte-small_rerank:False_reader-model:zephyr-7b-beta</td>\n",
       "      <td>1</td>\n",
       "      <td>Feedback: The response provided is incorrect based on the reference answer and the context given. The reference answer specifies the use of the `lm_eval` command with specific parameters for running the evaluation harness, while the response given details the use of a `litgpt evaluate` command with different parameters. This indicates a misunderstanding or misinterpretation of the question or the documents provided. The response fails to accurately reflect the correct command (`lm_eval`) and parameters as outlined in the reference answer and supported by the context documents. Therefore, the response does not meet the criteria for being correct, accurate, and factual in relation to the reference answer.</td>\n",
       "      <td>5</td>\n",
       "      <td>The response correctly identifies the command to run the evaluation harness as `litgpt evaluate` with the appropriate flags and parameters. It provides a clear and accurate template for the command, including placeholders for the model name, batch size, and tasks, as well as optional flags. The response also correctly notes the automatic conversion feature of the `litgpt evaluate` command and the use of the `--force_conversion` flag when necessary. The provided command matches the context given in the documents, specifically Document 4 and Document 5, which detail the use of `litgpt evaluate` for running the Evaluation Harness. Therefore, the response is completely correct, accurate, and factual based on the reference answer and the context provided.</td>\n",
       "      <td>./output/rag_chunk:200_embeddings:thenlper~gte-small_rerank:False_reader-model:zephyr-7b-beta.json</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                               question  \\\n",
       "0  What is the memory usage of Llama 2 with 7B when using bnb.nf4-dq?\\n   \n",
       "1                  What is the command to run the evaluation harness?\\n   \n",
       "\n",
       "                                                                                                                                                                                                                     true_answer  \\\n",
       "0                                                                                                                                                                                                                       13.84 GB   \n",
       "1  The command to run the evaluation harness is `lm_eval --model hf --model_args pretrained=out/hf-tinyllama/converted --tasks \"hellaswag,gsm8k,truthfulqa_mc2,mmlu,winogrande,arc_challenge\" --device \"cuda:0\" --batch_size 4`.   \n",
       "\n",
       "                        source_doc  \\\n",
       "0     tutorials/resource-tables.md   \n",
       "1  tutorials/convert_lit_models.md   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           generated_answer  \\\n",
       "0  \\n<|system|>\\nUsing the information contained in the context,\\ngive a comprehensive answer to the question.\\nRespond only to the question asked, response should be concise and relevant to the question.\\nProvide the number of the source document when relevant.\\nIf the answer cannot be deduced from the context, do not give an answer.</s>\\n<|user|>\\nContext:\\n\\nExtracted documents:\\nDocument 0:::\\n|       |                |              |                 |                      |             |                    |\\n| 7 B   | Llama 2        | None         | 1               | 4,194,304            | 21.30 GB    | 2.36 min           |\\n| 7 B   | Llama 2        | bnb.nf4      | 1               | 4,194,304            | 14.14 GB    | 3.68 min           |\\n| 7 B   | Llama 2        | bnb.nf4-dq   | 1               | 4,194,304            | 13.84 GB    | 3.83 min           |\\n| 7 B   | Llama 2        | None         | 2               | 4,194,304            | 29.07 GB    | 2.52 min           |\\n| 7 B   | Llama 2        | None         | 4               | 4,194,304            | OOM         | -                  |\\n|       |                |              |                 |                      |             |                    |Document 1:::\\n|       |                |              |          |                                           |           |\\n| 7 B   | Llama 2        | None         | 1 x A100 | 13.52 GB                                  | 30.97     |\\n| 7 B   | Llama 2        | bnb.nf4      | 1 x A100 | 4.57 GB                                   | 19.98     |\\n| 7 B   | Llama 2        | bnb.nf4-dq   | 1 x A100 | 4.26 GB                                   | 17.3      |\\n|       |                |              |          |                                           |           |\\n| 13 B  | Llama 2        | None         | 1 x A100 | 26.21 GB                                  | 24.82     |\\n| 13 B  | Llama 2        | bnb.nf4      | 1 x A100 | 8.32 GB                                   | 16.73     |Document 2:::\\n|       |                |              |                 |                      |             |                    |\\n| 13 B  | Llama 2        | None         | 1               | 6,553,600            | 38.12 GB    | 3.19 min           |\\n| 13 B  | Llama 2        | bnb.nf4      | 1               | 6,553,600            | 23.14 GB    | 6.38 min           |\\n| 13 B  | Llama 2        | bnb.nf4-dq   | 1               | 6,553,600            | 22.55 GB    | 6.55 min           |\\n| 13 B  | Llama 2        | None         | 2               | 6,553,600            | OOM         | -                  |\\n| 13 B  | Llama 2        | None         | 4               | 6,553,600            | OOM         | -                  |\\n|       |                |              |                 |                      |             |                    |Document 3:::\\n| 13 B  | Llama 2        | bnb.nf4-dq   | 1 x A100 | 7.72 GB                                   | 14.43     |\\n|       |                |              |          |                                           |           |\\n| 34 B  | CodeLlama      | None         | 1 x A100 | OOM                                       | -         |\\n| 34 B  | CodeLlama      | bnb.nf4      | 1 x A100 | 20.52 GB                                  | 14.32     |\\n| 34 B  | CodeLlama      | bnb.nf4-dq   | 1 x A100 | 18.95 GB                                  | 12.37     |\\n|       |                |              |          |                                           |           |\\n| 40 B  | Falcon         | None         | 1 x A100 | OOM                                       | -         |\\n| 40 B  | Falcon         | bnb.nf4      | 1 x A100 | 26.55 GB                                  | 13.25     |Document 4:::\\n| 13 B  | Llama 2        | bnb.nf4      | 1               | 6,553,600            | 2 x A100 | N/A         | -                  |\\n| 13 B  | Llama 2        | bnb.nf4-dq   | 1               | 6,553,600            | 2 x A100 | N/A         | -                  |\\n|       |                |              |                 |                      |          |             |                    |\\n| 13 B  | Llama 2        | None         | 1               | 6,553,600            | 4 x A100 | 35.57 GB    | 10.25 min          |\\n| 40 B  | Falcon         | None         | 1               | 12,042,240           | 4 x A100 | OOM         | -                  |Document 5:::\\n| 3 B  | StableLM Alpha | bnb.nf4      | 1               | 2,125,248            | 7.41 GB     | 1.59 min           |\\n| 3 B  | StableLM Alpha | bnb.nf4-dq   | 1               | 2,125,248            | 7.25 GB     | 1.62 min           |\\n|      |                |              |                 |                      |             |                    |\\n| 7 B  | Llama 2        | None         | 1               | 4,279,744            | 25.51 GB    | 1.81 min           |\\n| 7 B  | Llama 2        | bnb.nf4      | 1               | 4,279,744            | 18.30 GB    | 3.23 min           |\\n| 7 B  | Llama 2        | bnb.nf4-dq   | 1               | 4,279,744            | 17.98 GB    | 3.32 min           |Document 6:::\\n| 3 B  | StableLM Alpha | bnb.nf4      | 1               | 573,888              | 5.65 GB     | 1.38 min           |\\n| 3 B  | StableLM Alpha | bnb.nf4-dq   | 1               | 573,888              | 5.48 GB     | 1.46 min           |\\n|      |                |              |                 |                      |             |                    |\\n| 7 B  | Llama 2        | None         | 1               | 1,229,760            | 19.98 GB    | 1.50 min           |\\n| 7 B  | Llama 2        | bnb.nf4      | 1               | 1,229,760            | 12.68 GB    | 2.93 min           |\\n| 7 B  | Llama 2        | bnb.nf4-dq   | 1               | 1,229,760            | 12.38 GB    | 3.00 min           |\\n---\\nNow here is the question you need to answer.\\n\\nQuestion: What is the memory usage of Llama 2 with 7B when using bnb.nf4-dq?\\n\\n</s>\\n<|assistant|>\\nThe memory usage of Llama 2 with 7B when using bnb.nf4-dq is 4,194,304 bytes, as shown in Document 0, row 3. However, in Document 1, row 3, it is also listed as 4,26 GB, which is equivalent to 36,864,000,000 bytes. This discrepancy may be due to differences in the way memory is reported or measured in these documents. In either case, the memory usage is significantly lower than when using bnb.nf4 (14.14 GB) or None (29.07 GB) in Document 0.   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \\n<|system|>\\nUsing the information contained in the context,\\ngive a comprehensive answer to the question.\\nRespond only to the question asked, response should be concise and relevant to the question.\\nProvide the number of the source document when relevant.\\nIf the answer cannot be deduced from the context, do not give an answer.</s>\\n<|user|>\\nContext:\\n\\nExtracted documents:\\nDocument 0:::\\nThen, we can run the Evaluation Harness as follows:\\n\\n```bash\\nlm_eval --model hf \\\\n    --model_args pretrained=\"out/converted_model\" \\\\n    --tasks \"hellaswag,gsm8k,truthfulqa_mc2,mmlu,winogrande,arc_challenge\" \\\\n    --device \"cuda:0\" \\\\n    --batch_size 4\\n```\\n\\n&nbsp;\\n\\n> [!TIP]\\n> The Evaluation Harness tasks above are those used in Open LLM Leaderboard. You can find a list all supported tasks [here](https://github.com/EleutherAI/lm-evaluation-harness/blob/master/docs/task_table.md).Document 1:::\\n4. Run the evaluation harness, for example:\\n\\n```bash\\nlm_eval --model hf \\\\n    --model_args pretrained=out/hf-tinyllama/converted \\\\n    --tasks \"hellaswag,gsm8k,truthfulqa_mc2,mmlu,winogrande,arc_challenge\" \\\\n    --device \"cuda:0\" \\\\n    --batch_size 4\\n```Document 2:::\\n# LLM Evaluation\\n\\n&nbsp;\\n\\n## Using lm-evaluation-harness\\n\\nYou can evaluate LitGPT using [EleutherAI's lm-eval](https://github.com/EleutherAI/lm-evaluation-harness) framework with a large number of different evaluation tasks.\\n\\nYou need to install the `lm-eval` framework first:\\n\\n```bash\\npip install lm_eval\\n```\\n\\n&nbsp;\\n\\n### Evaluating LitGPT base models\\n\\nSuppose you downloaded a base model that we want to evaluate. Here, we use the `microsoft/phi-2` model:\\n\\n```bash\\nlitgpt download --repo_id microsoft/phi-2\\n```Document 3:::\\nYou can then use the model with external tools, for example, Eleuther AI's [LM Evaluation Harness](https://github.com/EleutherAI/lm-evaluation-harness) (see the `lm_eval` installation instructions [here](https://github.com/EleutherAI/lm-evaluation-harness?tab=readme-ov-file#install)).\\n\\nThe LM Evaluation Harness requires a tokenizer to be present in the model checkpoint folder, which we can copy from the original download checkpoint:\\n\\n```bash\\n# Copy the tokenizer needed by the Eval Harness\\ncp checkpoints/microsoft/phi-2/tokenizer*\\nout/converted_model\\n```\\n\\nThen, we can run the Evaluation Harness as follows:Document 4:::\\n&nbsp;\\n## Evaluating models\\n\\nLitGPT comes with a handy `litgpt evaluate` command to evaluate models with [Eleuther AI's Evaluation Harness](https://github.com/EleutherAI/lm-evaluation-harness). For example, to evaluate the previously downloaded `microsoft/phi-2` model on several tasks available from the Evaluation Harness, you can use the following command:\\n\\n```bash\\nlitgpt evaluate \\\\n  --checkpoint_dir checkpoints/microsoft/phi-2\\n  --batch_size 16 \\\\n  --tasks \"hellaswag,gsm8k,truthfulqa_mc2,mmlu,winogrande,arc_challenge\"\\n```Document 5:::\\nPlease note that the `litgpt evaluate` command run an internal model conversion. \\nThis is only necessary the first time you want to evaluate a model, and it will skip the\\nconversion steps if you run the `litgpt evaluate` on the same checkpint directory again.\\n\\nIn some cases, for example, if you modified the model in the `checkpoint_dir` since the first `litgpt evaluate`\\ncall, you need to use the `--force_conversion` flag to to update the files used by litgpt evaluate accordingly: \\n\\n```\\nlitgpt evaluate \\\\n  --checkpoint_dir checkpoints/microsoft/phi-2/ \\\\n  --batch_size 4 \\\\n  --out_dir evaluate_model/ \\\\n  --tasks \"hellaswag,truthfulqa_mc2,mmlu\" \\\\n  --force_conversion true\\n```Document 6:::\\n(A list of supported tasks can be found [here](https://github.com/EleutherAI/lm-evaluation-harness/blob/master/docs/task_table.md).)\\n\\n\\n&nbsp;\\n## Deploy LLMs\\n\\nYou can deploy LitGPT LLMs using your tool of choice. Below is an example using LitGPT built-in serving capabilities:\\n\\n\\n```bash\\n# 1) Download a pretrained model (alternatively, use your own finetuned model)\\nlitgpt download --repo_id microsoft/phi-2\\n\\n# 2) Start the server\\nlitgpt serve --checkpoint_dir checkpoints/microsoft/phi-2\\n```\\n---\\nNow here is the question you need to answer.\\n\\nQuestion: What is the command to run the evaluation harness?\\n\\n</s>\\n<|assistant|>\\nThe command to run the evaluation harness using LitGPT is:\\n\\n```bash\\nlitgpt evaluate \\\\n  --checkpoint_dir checkpoints/[MODEL_NAME] \\\\n  --batch_size [BATCH_SIZE] \\\\n  --tasks \"[TASK_1],[TASK_2],[TASK_3]\" \\\\n  [OPTIONAL_FLAGS]\\n```\\n\\nReplace `[MODEL_NAME]` with the name of the pretrained model you want to evaluate, `[BATCH_SIZE]` with the desired batch size, `[TASK_1],[TASK_2],[TASK_3]` with the names of the evaluation tasks you want to run (separated by commas), and `[OPTIONAL_FLAGS]` with any additional flags you want to pass to the `litgpt evaluate` command, such as `--out_dir` to specify an output directory or `--force_conversion` to force model conversion.\\n\\nNote that the `litgpt evaluate` command automatically converts the model for use with the Evaluation Harness if it hasn't been done before. If you modify the model in the `checkpoint_dir` since the last `litgpt evaluate` call, you can use the `--force_conversion` flag to update the files used by `litgpt evaluate`.\\n\\nA list of supported evaluation tasks can be found at https://github.com/EleutherAI/lm-evaluation-harness/blob/master/docs/task_table.md.   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           retrieved_docs  \\\n",
       "0  [|       |                |              |                 |                      |             |                    |\\n| 7 B   | Llama 2        | None         | 1               | 4,194,304            | 21.30 GB    | 2.36 min           |\\n| 7 B   | Llama 2        | bnb.nf4      | 1               | 4,194,304            | 14.14 GB    | 3.68 min           |\\n| 7 B   | Llama 2        | bnb.nf4-dq   | 1               | 4,194,304            | 13.84 GB    | 3.83 min           |\\n| 7 B   | Llama 2        | None         | 2               | 4,194,304            | 29.07 GB    | 2.52 min           |\\n| 7 B   | Llama 2        | None         | 4               | 4,194,304            | OOM         | -                  |\\n|       |                |              |                 |                      |             |                    |, |       |                |              |          |                                           |           |\\n| 7 B   | Llama 2        | None         | 1 x A100 | 13.52 GB                                  | 30.97     |\\n| 7 B   | Llama 2        | bnb.nf4      | 1 x A100 | 4.57 GB                                   | 19.98     |\\n| 7 B   | Llama 2        | bnb.nf4-dq   | 1 x A100 | 4.26 GB                                   | 17.3      |\\n|       |                |              |          |                                           |           |\\n| 13 B  | Llama 2        | None         | 1 x A100 | 26.21 GB                                  | 24.82     |\\n| 13 B  | Llama 2        | bnb.nf4      | 1 x A100 | 8.32 GB                                   | 16.73     |, |       |                |              |                 |                      |             |                    |\\n| 13 B  | Llama 2        | None         | 1               | 6,553,600            | 38.12 GB    | 3.19 min           |\\n| 13 B  | Llama 2        | bnb.nf4      | 1               | 6,553,600            | 23.14 GB    | 6.38 min           |\\n| 13 B  | Llama 2        | bnb.nf4-dq   | 1               | 6,553,600            | 22.55 GB    | 6.55 min           |\\n| 13 B  | Llama 2        | None         | 2               | 6,553,600            | OOM         | -                  |\\n| 13 B  | Llama 2        | None         | 4               | 6,553,600            | OOM         | -                  |\\n|       |                |              |                 |                      |             |                    |, | 13 B  | Llama 2        | bnb.nf4-dq   | 1 x A100 | 7.72 GB                                   | 14.43     |\\n|       |                |              |          |                                           |           |\\n| 34 B  | CodeLlama      | None         | 1 x A100 | OOM                                       | -         |\\n| 34 B  | CodeLlama      | bnb.nf4      | 1 x A100 | 20.52 GB                                  | 14.32     |\\n| 34 B  | CodeLlama      | bnb.nf4-dq   | 1 x A100 | 18.95 GB                                  | 12.37     |\\n|       |                |              |          |                                           |           |\\n| 40 B  | Falcon         | None         | 1 x A100 | OOM                                       | -         |\\n| 40 B  | Falcon         | bnb.nf4      | 1 x A100 | 26.55 GB                                  | 13.25     |, | 13 B  | Llama 2        | bnb.nf4      | 1               | 6,553,600            | 2 x A100 | N/A         | -                  |\\n| 13 B  | Llama 2        | bnb.nf4-dq   | 1               | 6,553,600            | 2 x A100 | N/A         | -                  |\\n|       |                |              |                 |                      |          |             |                    |\\n| 13 B  | Llama 2        | None         | 1               | 6,553,600            | 4 x A100 | 35.57 GB    | 10.25 min          |\\n| 40 B  | Falcon         | None         | 1               | 12,042,240           | 4 x A100 | OOM         | -                  |, | 3 B  | StableLM Alpha | bnb.nf4      | 1               | 2,125,248            | 7.41 GB     | 1.59 min           |\\n| 3 B  | StableLM Alpha | bnb.nf4-dq   | 1               | 2,125,248            | 7.25 GB     | 1.62 min           |\\n|      |                |              |                 |                      |             |                    |\\n| 7 B  | Llama 2        | None         | 1               | 4,279,744            | 25.51 GB    | 1.81 min           |\\n| 7 B  | Llama 2        | bnb.nf4      | 1               | 4,279,744            | 18.30 GB    | 3.23 min           |\\n| 7 B  | Llama 2        | bnb.nf4-dq   | 1               | 4,279,744            | 17.98 GB    | 3.32 min           |, | 3 B  | StableLM Alpha | bnb.nf4      | 1               | 573,888              | 5.65 GB     | 1.38 min           |\\n| 3 B  | StableLM Alpha | bnb.nf4-dq   | 1               | 573,888              | 5.48 GB     | 1.46 min           |\\n|      |                |              |                 |                      |             |                    |\\n| 7 B  | Llama 2        | None         | 1               | 1,229,760            | 19.98 GB    | 1.50 min           |\\n| 7 B  | Llama 2        | bnb.nf4      | 1               | 1,229,760            | 12.68 GB    | 2.93 min           |\\n| 7 B  | Llama 2        | bnb.nf4-dq   | 1               | 1,229,760            | 12.38 GB    | 3.00 min           |]   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  [Then, we can run the Evaluation Harness as follows:\\n\\n```bash\\nlm_eval --model hf \\\\n    --model_args pretrained=\"out/converted_model\" \\\\n    --tasks \"hellaswag,gsm8k,truthfulqa_mc2,mmlu,winogrande,arc_challenge\" \\\\n    --device \"cuda:0\" \\\\n    --batch_size 4\\n```\\n\\n&nbsp;\\n\\n> [!TIP]\\n> The Evaluation Harness tasks above are those used in Open LLM Leaderboard. You can find a list all supported tasks [here](https://github.com/EleutherAI/lm-evaluation-harness/blob/master/docs/task_table.md)., 4. Run the evaluation harness, for example:\\n\\n```bash\\nlm_eval --model hf \\\\n    --model_args pretrained=out/hf-tinyllama/converted \\\\n    --tasks \"hellaswag,gsm8k,truthfulqa_mc2,mmlu,winogrande,arc_challenge\" \\\\n    --device \"cuda:0\" \\\\n    --batch_size 4\\n```, # LLM Evaluation\\n\\n&nbsp;\\n\\n## Using lm-evaluation-harness\\n\\nYou can evaluate LitGPT using [EleutherAI's lm-eval](https://github.com/EleutherAI/lm-evaluation-harness) framework with a large number of different evaluation tasks.\\n\\nYou need to install the `lm-eval` framework first:\\n\\n```bash\\npip install lm_eval\\n```\\n\\n&nbsp;\\n\\n### Evaluating LitGPT base models\\n\\nSuppose you downloaded a base model that we want to evaluate. Here, we use the `microsoft/phi-2` model:\\n\\n```bash\\nlitgpt download --repo_id microsoft/phi-2\\n```, You can then use the model with external tools, for example, Eleuther AI's [LM Evaluation Harness](https://github.com/EleutherAI/lm-evaluation-harness) (see the `lm_eval` installation instructions [here](https://github.com/EleutherAI/lm-evaluation-harness?tab=readme-ov-file#install)).\\n\\nThe LM Evaluation Harness requires a tokenizer to be present in the model checkpoint folder, which we can copy from the original download checkpoint:\\n\\n```bash\\n# Copy the tokenizer needed by the Eval Harness\\ncp checkpoints/microsoft/phi-2/tokenizer*\\nout/converted_model\\n```\\n\\nThen, we can run the Evaluation Harness as follows:, &nbsp;\\n## Evaluating models\\n\\nLitGPT comes with a handy `litgpt evaluate` command to evaluate models with [Eleuther AI's Evaluation Harness](https://github.com/EleutherAI/lm-evaluation-harness). For example, to evaluate the previously downloaded `microsoft/phi-2` model on several tasks available from the Evaluation Harness, you can use the following command:\\n\\n```bash\\nlitgpt evaluate \\\\n  --checkpoint_dir checkpoints/microsoft/phi-2\\n  --batch_size 16 \\\\n  --tasks \"hellaswag,gsm8k,truthfulqa_mc2,mmlu,winogrande,arc_challenge\"\\n```, Please note that the `litgpt evaluate` command run an internal model conversion. \\nThis is only necessary the first time you want to evaluate a model, and it will skip the\\nconversion steps if you run the `litgpt evaluate` on the same checkpint directory again.\\n\\nIn some cases, for example, if you modified the model in the `checkpoint_dir` since the first `litgpt evaluate`\\ncall, you need to use the `--force_conversion` flag to to update the files used by litgpt evaluate accordingly: \\n\\n```\\nlitgpt evaluate \\\\n  --checkpoint_dir checkpoints/microsoft/phi-2/ \\\\n  --batch_size 4 \\\\n  --out_dir evaluate_model/ \\\\n  --tasks \"hellaswag,truthfulqa_mc2,mmlu\" \\\\n  --force_conversion true\\n```, (A list of supported tasks can be found [here](https://github.com/EleutherAI/lm-evaluation-harness/blob/master/docs/task_table.md).)\\n\\n\\n&nbsp;\\n## Deploy LLMs\\n\\nYou can deploy LitGPT LLMs using your tool of choice. Below is an example using LitGPT built-in serving capabilities:\\n\\n\\n```bash\\n# 1) Download a pretrained model (alternatively, use your own finetuned model)\\nlitgpt download --repo_id microsoft/phi-2\\n\\n# 2) Start the server\\nlitgpt serve --checkpoint_dir checkpoints/microsoft/phi-2\\n```]   \n",
       "\n",
       "                                                                      test_settings  \\\n",
       "0  chunk:200_embeddings:thenlper~gte-small_rerank:False_reader-model:zephyr-7b-beta   \n",
       "1  chunk:200_embeddings:thenlper~gte-small_rerank:False_reader-model:zephyr-7b-beta   \n",
       "\n",
       "  eval_score_GPT4_0125  \\\n",
       "0                    1   \n",
       "1                    1   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    eval_feedback_GPT4_0125  \\\n",
       "0                                                                                                                                                   Feedback: The response provided incorrect information by stating the memory usage in bytes and then incorrectly converting those bytes into gigabytes, leading to a significant discrepancy. The correct answer, as per the reference answer, is 13.84 GB, which is directly mentioned in the provided context (Document 0, row 3). The response failed to accurately report this figure and instead introduced confusion with incorrect data conversion. This deviation from the factual content in the context and the reference answer indicates a lack of accuracy in the response.   \n",
       "1  Feedback: The response provided is incorrect based on the reference answer and the context given. The reference answer specifies the use of the `lm_eval` command with specific parameters for running the evaluation harness, while the response given details the use of a `litgpt evaluate` command with different parameters. This indicates a misunderstanding or misinterpretation of the question or the documents provided. The response fails to accurately reflect the correct command (`lm_eval`) and parameters as outlined in the reference answer and supported by the context documents. Therefore, the response does not meet the criteria for being correct, accurate, and factual in relation to the reference answer.   \n",
       "\n",
       "  eval_score_GPT4_1106  \\\n",
       "0                    1   \n",
       "1                    5   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    eval_feedback_GPT4_1106  \\\n",
       "0                                                                                                                                                 The response provided is incorrect and not factual as per the reference answer. The response cites two different memory usages, 4,194,304 bytes and 4.26 GB, which are both incorrect and do not match the reference answer of 13.84 GB. The correct memory usage is clearly stated in the context provided in Document 0, row 3, which should have been the sole focus of the answer. The response also introduces unnecessary confusion by mentioning a discrepancy without clarifying that the correct answer is 13.84 GB as per the reference answer. Therefore, the response does not meet the criteria for accuracy and factuality.   \n",
       "1  The response correctly identifies the command to run the evaluation harness as `litgpt evaluate` with the appropriate flags and parameters. It provides a clear and accurate template for the command, including placeholders for the model name, batch size, and tasks, as well as optional flags. The response also correctly notes the automatic conversion feature of the `litgpt evaluate` command and the use of the `--force_conversion` flag when necessary. The provided command matches the context given in the documents, specifically Document 4 and Document 5, which detail the use of `litgpt evaluate` for running the Evaluation Harness. Therefore, the response is completely correct, accurate, and factual based on the reference answer and the context provided.   \n",
       "\n",
       "                                                                                             settings  \n",
       "0  ./output/rag_chunk:200_embeddings:thenlper~gte-small_rerank:False_reader-model:zephyr-7b-beta.json  \n",
       "1  ./output/rag_chunk:200_embeddings:thenlper~gte-small_rerank:False_reader-model:zephyr-7b-beta.json  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "result[\"eval_score_GPT4_0125\"] = result[\"eval_score_GPT4_0125\"].apply(\n",
    "    lambda x: int(x) if isinstance(x, str) else 1\n",
    ")\n",
    "result[\"eval_score_GPT4_0125\"] = (result[\"eval_score_GPT4_0125\"] - 1) / 4\n",
    "\n",
    "result[\"eval_score_GPT4_1106\"] = result[\"eval_score_GPT4_1106\"].apply(\n",
    "    lambda x: int(x) if isinstance(x, str) else 1\n",
    ")\n",
    "result[\"eval_score_GPT4_1106\"] = (result[\"eval_score_GPT4_1106\"] - 1) / 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "settings\n",
       "./output/rag_chunk:200_embeddings:thenlper~gte-small_rerank:False_reader-model:zephyr-7b-beta.json        0.758333\n",
       "./output/rag_chunk:200_embeddings:BAAI~bge-small-en-v1.5_rerank:False_reader-model:zephyr-7b-beta.json    0.800000\n",
       "./output/rag_chunk:200_embeddings:BAAI~bge-small-en-v1.5_rerank:True_reader-model:zephyr-7b-beta.json     0.800000\n",
       "./output/rag_chunk:200_embeddings:thenlper~gte-small_rerank:True_reader-model:zephyr-7b-beta.json         0.866667\n",
       "Name: eval_score_GPT4_1106, dtype: float64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_scores = result.groupby(\"settings\")[\"eval_score_GPT4_1106\"].mean()\n",
    "average_scores.sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "settings\n",
       "./output/rag_chunk:200_embeddings:thenlper~gte-small_rerank:False_reader-model:zephyr-7b-beta.json        0.725000\n",
       "./output/rag_chunk:200_embeddings:BAAI~bge-small-en-v1.5_rerank:False_reader-model:zephyr-7b-beta.json    0.816667\n",
       "./output/rag_chunk:200_embeddings:BAAI~bge-small-en-v1.5_rerank:True_reader-model:zephyr-7b-beta.json     0.816667\n",
       "./output/rag_chunk:200_embeddings:thenlper~gte-small_rerank:True_reader-model:zephyr-7b-beta.json         0.858333\n",
       "Name: eval_score_GPT4_0125, dtype: float64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_scores = result.groupby(\"settings\")[\"eval_score_GPT4_0125\"].mean()\n",
    "average_scores.sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cloudspace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
